{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":123952,"status":"ok","timestamp":1679918601060,"user":{"displayName":"Ricupuch","userId":"12829564591003013695"},"user_tz":-120},"id":"u_8PYUQi5FQS","outputId":"f4a7f0c4-efb9-48dc-e207-8155b82b65e4"},"outputs":[],"source":["# important: update to torch 2.0 s.t. pre_hooks are available\n","# !pip install torch torchvision -U\n","# !pip install git+https://github.com/huggingface/transformers\n","# !pip install datasets evaluate"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1679917884975,"user":{"displayName":"Ricupuch","userId":"12829564591003013695"},"user_tz":-120},"id":"dw8ugrCeiNfs","outputId":"8dd00d8d-49b5-4b4d-9372-1802712d6e0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch Version:  2.0.0\n","Torchvision Version:  0.15.1+cpu\n"]}],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader,TensorDataset,random_split\n","from torchvision import datasets, transforms\n","\n","import matplotlib.pyplot as plt\n","import torchvision\n","import numpy as np\n","import time, os, copy, random\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)"]},{"cell_type":"markdown","metadata":{"id":"SjqoNbkwixux"},"source":["# Create artifical data"]},{"cell_type":"markdown","metadata":{"id":"CbKbEy-8dGUv"},"source":["Set random seeds."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Mn7f1HwtwAJd"},"outputs":[],"source":["torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)"]},{"cell_type":"markdown","metadata":{"id":"QorOfAM8dIUY"},"source":["Set the number of batches and the batch size. For these early tests, 5 batches of size 3 should give good insights while not being to complicated."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gA-mAFABomFA"},"outputs":[],"source":["n_batches = 10\n","batch_size = 3\n","n_samples = n_batches * batch_size"]},{"cell_type":"markdown","metadata":{"id":"w7LJIGvLdbll"},"source":["The network architecture, initial weights and test data is similar to this source:\n","https://www.kaggle.com/code/sironghuang/understanding-pytorch-hooks.\n","\n","In the linked notebook, only one datapoint is evaluated. Here, this datapoint will be repeated to include the effects of using batches."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":120,"status":"ok","timestamp":1679487416738,"user":{"displayName":"Ricupuch","userId":"12829564591003013695"},"user_tz":-60},"id":"jsDhED5Ai0Ed","outputId":"832cfada-b9a2-4d13-95dc-1bfd551beccd"},"outputs":[{"name":"stdout","output_type":"stream","text":["dataset size :torch.Size([30, 2])\n","single sample, size: torch.Size([2]) | values: tensor([-0.0075,  0.5364])\n"]}],"source":["artificial_data = 2*(torch.rand((n_samples,2))-1/2)\n","print(f'dataset size :{artificial_data.shape}')\n","print(f'single sample, size: {artificial_data[0,:].shape} | values: {artificial_data[0,:]}')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114,"status":"ok","timestamp":1679487416738,"user":{"displayName":"Ricupuch","userId":"12829564591003013695"},"user_tz":-60},"id":"vbSsuujJ7CsS","outputId":"2330c2e5-d081-4ab8-8b21-3a0afd07132e"},"outputs":[{"name":"stdout","output_type":"stream","text":["label set size :torch.Size([30, 1])\n","single label, size: torch.Size([1]) | values: tensor([1], dtype=torch.int32)\n"]}],"source":["artifical_labels = ((artificial_data[:,0]**2 + artificial_data[:,1]**2) < 0.7).int().unsqueeze(1)\n","print(f'label set size :{artifical_labels.shape}')\n","print(f'single label, size: {artifical_labels[0,:].shape} | values: {artifical_labels[0,:]}')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["<matplotlib.collections.PathCollection at 0x28117c8c490>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 600x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["colors = {0:'orange',1:'blue'}\n","mapped_colors = [colors[label[0]] for label in artifical_labels.tolist()]\n","fig, ax = plt.subplots(figsize=(6, 6))\n","ax.scatter(artificial_data.numpy()[:,0],artificial_data.numpy()[:,1],c=mapped_colors)\n"]},{"cell_type":"markdown","metadata":{"id":"6eY_iCiAgdEH"},"source":["Next, the datasets and dataloader are created from the tensors. The first 4*batch_size samples are being used as the training set and the remaining batch_size samples are the test set. The splitting of datasets is not necessary for now but will make extension easy later on.\n","\n","Tensordata requires a 2D tensor, where each line represents one training sample. Targets may be 1-D or 2-D."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":107,"status":"ok","timestamp":1679487416739,"user":{"displayName":"Ricupuch","userId":"12829564591003013695"},"user_tz":-60},"id":"4j4s1b5ovWul","outputId":"175624ab-d1ba-416b-847b-dac8f0cb2360"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of datapoints in the training set is 12\n"]}],"source":["train_set = TensorDataset(artificial_data[:4*batch_size,], artifical_labels[:4*batch_size,])\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False)\n","print(f'Number of datapoints in the training set is {len(train_set)}')"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":103,"status":"ok","timestamp":1679487416741,"user":{"displayName":"Ricupuch","userId":"12829564591003013695"},"user_tz":-60},"id":"dvcyG4rszTr9","outputId":"dc4f7026-5f5e-471e-8bac-f7c89625e22c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of datapoints in the evaluation set is 18\n"]}],"source":["eval_set = TensorDataset(artificial_data[4*batch_size:,], artifical_labels[4*batch_size:,])\n","eval_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False)\n","print(f'Number of datapoints in the evaluation set is {len(eval_set)}')"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"rwHDPE_ezrrK"},"outputs":[],"source":["dataloaders = {'train':train_loader,\n","               'eval':eval_loader}"]},{"cell_type":"markdown","metadata":{"id":"o0iH0l_Fj7e1"},"source":["# Create sample model"]},{"cell_type":"markdown","metadata":{"id":"IAA6XFQrXyd_"},"source":["The base model architecture and weights are taken from [here](https://www.kaggle.com/code/sironghuang/understanding-pytorch-hooks) for reference.\n","\n","Here, the architecture is extended by a dropout layer."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"8aSvRv4aj-jN"},"outputs":[],"source":["class TestModel(nn.Module):\n","  def __init__(self, dropout_rate = 0.5):\n","        super().__init__()\n","        self.fc1 = nn.Linear(2,2)\n","        self.s1 = nn.Sigmoid()\n","        self.do1 = nn.Dropout(p=dropout_rate)\n","        self.fc2 = nn.Linear(2,1)\n","        self.fc1.weight = torch.nn.Parameter(torch.Tensor([[0.15,0.2],[0.250,0.30]]))\n","        self.fc1.bias = torch.nn.Parameter(torch.Tensor([0.35]))\n","        self.fc2.weight = torch.nn.Parameter(torch.Tensor([[0.4,0.5]]))\n","        self.fc2.bias = torch.nn.Parameter(torch.Tensor([0.6]))\n","\n","  def forward(self, x):\n","      x= self.fc1(x)\n","      x = self.s1(x)\n","      x = self.do1(x)\n","      x= self.fc2(x)\n","      return x"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92,"status":"ok","timestamp":1679487416744,"user":{"displayName":"Ricupuch","userId":"12829564591003013695"},"user_tz":-60},"id":"QRCU-Okzm2NS","outputId":"768dcf91-b0b7-468d-e8cd-068048ebc022"},"outputs":[{"name":"stdout","output_type":"stream","text":["TestModel(\n","  (fc1): Linear(in_features=2, out_features=2, bias=True)\n","  (s1): Sigmoid()\n","  (do1): Dropout(p=0.5, inplace=False)\n","  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",")\n"]}],"source":["model = TestModel()\n","print(model)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87,"status":"ok","timestamp":1679487416745,"user":{"displayName":"Ricupuch","userId":"12829564591003013695"},"user_tz":-60},"id":"MMae-esXuBlq","outputId":"c4021d1b-5ab0-4f8a-a27e-425e2679f9a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Layer: fc1.weight | Size: torch.Size([2, 2]) | Values : tensor([[0.1500, 0.2000],\n","        [0.2500, 0.3000]], grad_fn=<SliceBackward0>) \n","\n","Layer: fc1.bias | Size: torch.Size([1]) | Values : tensor([0.3500], grad_fn=<SliceBackward0>) \n","\n","Layer: fc2.weight | Size: torch.Size([1, 2]) | Values : tensor([[0.4000, 0.5000]], grad_fn=<SliceBackward0>) \n","\n","Layer: fc2.bias | Size: torch.Size([1]) | Values : tensor([0.6000], grad_fn=<SliceBackward0>) \n","\n"]}],"source":["for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"80KDZLJws763"},"source":["# Prepare optimizer and loss function"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"EltLsz4_tYEN"},"outputs":[],"source":["sgd_parameters = {\n","    'lr':1e-3,        # undefined\n","    'momentum':0,   # 0\n","    'dampening':0,    # 0\n","    'weight_decay':0  # 0\n","}\n","optimizer = torch.optim.SGD(model.parameters(), **sgd_parameters)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"B3US8RnNthm-"},"outputs":[],"source":["loss_fn = nn.BCEWithLogitsLoss()"]},{"cell_type":"markdown","metadata":{"id":"pyeXQRZ5tf8z"},"source":["# Hooks"]},{"cell_type":"markdown","metadata":{"id":"EF6CyBtEth72"},"source":["Create two hooks for debugging purposes:\n","- the forward hook will print the input and output tensor produced during the forward pass.\n","- the backward hook will print the gradient of the output (the gradient coming from the loss) and the gradient input (the gradient used for following calculations closer to the input layers) during the backward pass. There is a print_eval boolean to determine, if the debugging is only supposed when the model is in eval() mode or only when in train() mode. This is to prevent verbose outputs of uninteresting layers during the unregularized run."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"_FaKBHjTte5A"},"outputs":[],"source":["def forward_debug_hook(module, input, output):\n","  print('forward hook')\n","  print(input)\n","  print(output)\n","\n","class Backward_Debug_Hook():\n","  def __init__(self, module, layer_name, print_eval=False):\n","    self.hook = module.register_full_backward_hook(self.hook_fn)\n","    self.module = module\n","    self.layer_name = layer_name\n","    self.print_eval = print_eval\n","\n","  def hook_fn(self, module, grad_input, grad_output):\n","    # if-statement checks, that hook is only executed during evaluation OR training\n","    if (self.module.training and not self.print_eval) or (not self.module.training and self.print_eval):\n","      print(f'layer: {self.layer_name} with:')\n","      print('grad_output')\n","      print(grad_output)\n","      print('grad_input')\n","      print(grad_input)\n","    \n","  def close(self):\n","    self.hook.remove() "]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":80,"status":"ok","timestamp":1679487416748,"user":{"displayName":"Ricupuch","userId":"12829564591003013695"},"user_tz":-60},"id":"xJ-eYpIP-vP8","outputId":"1ea1ffb7-0de3-4a79-e47e-714409814adb"},"outputs":[{"data":{"text/plain":["{'fc1_train': <__main__.Backward_Debug_Hook at 0x28118136d30>,\n"," 's1_train': <__main__.Backward_Debug_Hook at 0x28118136d60>,\n"," 'do1_train': <__main__.Backward_Debug_Hook at 0x28118136700>,\n"," 'fc2_train': <__main__.Backward_Debug_Hook at 0x28118136fa0>,\n"," 'fc2_eval': <__main__.Backward_Debug_Hook at 0x28118136d00>}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["debug_hook_dict = {}\n","affected_layer = model.fc2\n","affected_name = 'fc2'\n","\n","for name, module in model.named_modules():\n","  if not isinstance(module,TestModel):\n","    debug_hook_dict[name+'_train'] = Backward_Debug_Hook(module,name)\n","\n","debug_hook_dict[affected_name+'_eval'] = Backward_Debug_Hook(affected_layer,affected_name,print_eval=True)\n","debug_hook_dict"]},{"cell_type":"markdown","metadata":{"id":"DeJbEpNJtjDy"},"source":["Another hook is used to extract the unregularized gradient. The hook is supposed to catch the input gradient (the gradient being sent to the next layer) when reaching the layer before the logits layer (last fc layer of the model).\n","\n","For this, the hook is created as a class to store the gradient for later use. \n","\n","Note that the hook will be attached to the layer the whole time and the stored gradient will simply be overwritten.\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"ipquUx53tjyN"},"outputs":[],"source":["class Catch_Hook():\n","  def __init__(self, module):\n","    self.hook = module.register_full_backward_hook(self.hook_fn)\n","\n","  def hook_fn(self, module, grad_input, grad_output):\n","    self.caught_grad = grad_output\n","    print('caught a gradient:')\n","    print(self.caught_grad)\n","\n","  def close(self):\n","    self.hook.remove()\n","\n","catch_hook = Catch_Hook(affected_layer)"]},{"cell_type":"markdown","metadata":{"id":"t0iC6PnGtlxB"},"source":["Re-insertion of the gradient is a little bit more tricky.\n","\n","By using the return statement in the backward hook, the gradient can be manipulated. There are two possibilities:\n","1. using the full_backward_hook will insert the gradient in the return statement as the ***input gradient*** (see [here](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook))\n","2. using the full_backward_pre_hook will insert the gradient in the return statement as the ***output gradient*** (see [here](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook))\n","\n","Approach number 2 was not successfull (see remarks at the end of the notebook).\n","\n","In the logit case, we want to replace the gradient dLoss/dLogits, which is in this case excactly the input gradient of the preceding layer (the sigmoid layer). \n","\n","Additionally, we only want to apply this hook during the regularized run. For this the model.training boolean is used. In each batch, the new unregularized gradient is updated via the .update_grad() method."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"ht57TVZxtnyo"},"outputs":[],"source":["class Insert_Hook():\n","  def __init__(self, module, new_grad_output=None):\n","    self.new_grad_output = new_grad_output\n","    # use prepend=True so that this is definetly the first hook being applied\n","    self.hook = module.register_full_backward_pre_hook(self.hook_fn,prepend=True)\n","\n","  def hook_fn(self, module, grad_output):\n","    if module.training:\n","      print('inserted gradient:')\n","      print(self.new_grad_output)\n","      # simply return the previously caught grad_output\n","      # this will replace the current grad_output (if prehook is used)\n","      # if non-pre hook is used, grad_input will be replaced (not desire in our case)\n","      return self.new_grad_output\n","    else:\n","      print('skipping gradient insertion in eval mode')\n","\n","  def update_grad(self, new_grad_output):\n","    self.new_grad_output = new_grad_output\n","\n","  def close(self):\n","    self.hook.remove()\n","\n","# artifical_grad = (100*torch.ones([3,2]),)\n","artifical_grad = None\n","\n","insert_hook = Insert_Hook(affected_layer,artifical_grad)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Important note\n","The implementation of full_backward_pre_hook contains a bug in torch v.2.0.0 (see here: https://discuss.pytorch.org/t/understanding-gradient-calculation-with-backward-pre-hooks/176154). A fix will be made available in a nightly build soon. Till then, the fix can be applied on the local machine. I have not made the effort to transfer this fix to the colab workspace."]},{"cell_type":"markdown","metadata":{"id":"tghqFX7iWO6f"},"source":["If one wants to compare the training process with the insertion hook to the one without, simply uncomment the last line, where the insert_hook is being created."]},{"cell_type":"markdown","metadata":{"id":"IFVjXtIsuYNt"},"source":["# Model training"]},{"cell_type":"markdown","metadata":{"id":"2vnUDQ886Q7b"},"source":["Comments on the training loops are inside the code.\n","\n","The general idea for each batch is:\n","- Run the model in .eval() mode to deactivate dropout and run the backwards pass. Do NOT update the gradients via optimizer.step() and reset the gradients via optimizer.zero_grad(). Catch the desired gradient with the catch_hook.\n","\n","\n","\n","\n","\n","Caveats to this method:\n","- using model.eval() will make batchnorms use the rolling average instead of the actual batch distributions. This is not reasonable for this method. An option would be to set the dropout rates in the model to zero by hand and reset them after the unregularized run. Then one would not have to use .eval().\n","\n","- This approach requires us to do the forward and backward pass twice. This will increase the computational load. The memory consumption is only slighty bigger, as only the gradient of one layer has to be stored."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"5l3DWY0euaYR"},"outputs":[],"source":["def train_model(model, dataloaders, loss_fn, optimizer, num_epochs=5):\n","    since = time.time()\n","\n","    val_acc_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    n_train_batches = len(dataloaders['train'])\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        ########## train phase ##########\n","        phase = 'train'\n","        model.train()\n","\n","        running_loss = 0.0\n","        running_corrects = 0\n","\n","        for batch, (inputs, labels) in enumerate(dataloaders[phase]):\n","          \n","\n","          #++++++++ catch unregularized gradient ++++++++#\n","          optimizer.zero_grad()\n","          print('*'*5 + 'unregularized run' + '*'*5)\n","          # with torch.no_grad():\n","          #   model.eval()\n","          #   outputs = model(inputs)\n","          #   loss = loss_fn(outputs, labels.float())\n","          #   loss.backward()\n","\n","          model.eval()\n","          outputs = model(inputs)\n","          loss = loss_fn(outputs, labels.float())\n","          loss.backward()\n","          \n","          # the following grad computation requires output of the model to be the logits, not the probability.\n","          # manual_grad = torch.autograd.grad(loss,outputs,retain_graph=True)\n","          # print(f'calculated gradient d_L/d_logits: {manual_grad}')\n","\n","          \n","          #++++++++ \\catch unregularized gradient ++++++++#\n","\n","          #++++++++ update unregularized gradient to be inserted ++++++++#\n","          insert_hook.update_grad(catch_hook.caught_grad)\n","          # insert_hook.update_grad(manual_grad)\n","          #++++++++ \\update unregularized gradient to be inserted ++++++++#\n","          \n","          # Get model outputs and calculate loss\n","          model.train()\n","          optimizer.zero_grad()\n","          print('*'*5 + 'forward pass' + '*'*5)\n","          outputs = model(inputs)\n","          print('model outputs')\n","          print(outputs)\n","\n","\n","          print('*'*5 + 'loss calculation' + '*'*5)\n","          loss = loss_fn(outputs, labels.float())\n","          print(f'loss: {loss}')\n","\n","          preds = (outputs>0.5).int()\n","          \n","\n","          # backward + optimize\n","          print('*'*5 + 'backward pass' + '*'*5)\n","          loss.backward()          \n","          optimizer.step()\n","\n","          running_loss += loss.item() * inputs.size(0)\n","          running_corrects += torch.sum(preds == labels.data)\n","\n","        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","        epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","        ########## eval phase ##########\n","        phase = 'eval'\n","        model.eval()\n","\n","        running_loss = 0.0\n","        running_corrects = 0\n","\n","        for batch, (inputs, labels) in enumerate(dataloaders[phase]):\n","          # disable gradient tracking for speedup\n","          with torch.set_grad_enabled(phase == 'train'):\n","            outputs = model(inputs)\n","            loss = loss_fn(outputs, labels.float())\n","            preds = (outputs>0.5).int()\n","\n","          running_loss += loss.item() * inputs.size(0)\n","          running_corrects += torch.sum(preds == labels.data)\n","\n","        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","        epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))  \n","\n","        val_acc_history.append(epoch_acc)\n","        print()\n","\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","\n","    return val_acc_history"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74,"status":"ok","timestamp":1679487416752,"user":{"displayName":"Ricupuch","userId":"12829564591003013695"},"user_tz":-60},"id":"kLVefwl04gyl","outputId":"f0ff882c-7589-4b3f-921b-07f9fdafd666"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0/0\n","----------\n","*****unregularized run*****\n","skipping gradient insertion in eval mode\n","layer: fc2 with:\n","grad_output\n","(tensor([[-0.0797],\n","        [ 0.2468],\n","        [-0.0817]]),)\n","grad_input\n","(tensor([[-0.0319, -0.0399],\n","        [ 0.0987,  0.1234],\n","        [-0.0327, -0.0408]]),)\n","caught a gradient:\n","(tensor([[-0.0797],\n","        [ 0.2468],\n","        [-0.0817]]),)\n","*****forward pass*****\n","model outputs\n","tensor([[1.7143],\n","        [1.0809],\n","        [0.6000]], grad_fn=<BackwardHookFunctionBackward>)\n","*****loss calculation*****\n","loss: 0.6587010025978088\n","*****backward pass*****\n","inserted gradient:\n","(tensor([[-0.0797],\n","        [ 0.2468],\n","        [-0.0817]]),)\n","layer: fc2 with:\n","grad_output\n","(tensor([[-0.0797],\n","        [ 0.2468],\n","        [-0.0817]]),)\n","grad_input\n","(tensor([[-0.0319, -0.0399],\n","        [ 0.0987,  0.1234],\n","        [-0.0327, -0.0408]]),)\n","caught a gradient:\n","(tensor([[-0.0797],\n","        [ 0.2468],\n","        [-0.0817]]),)\n","layer: do1 with:\n","grad_output\n","(tensor([[-0.0319, -0.0399],\n","        [ 0.0987,  0.1234],\n","        [-0.0327, -0.0408]]),)\n","grad_input\n","(tensor([[-0.0638, -0.0797],\n","        [ 0.0000,  0.2468],\n","        [-0.0000, -0.0000]]),)\n","layer: s1 with:\n","grad_output\n","(tensor([[-0.0638, -0.0797],\n","        [ 0.0000,  0.2468],\n","        [-0.0000, -0.0000]]),)\n","grad_input\n","(tensor([[-0.0151, -0.0187],\n","        [ 0.0000,  0.0616],\n","        [-0.0000, -0.0000]]),)\n","layer: fc1 with:\n","grad_output\n","(tensor([[-0.0151, -0.0187],\n","        [ 0.0000,  0.0616],\n","        [-0.0000, -0.0000]]),)\n","grad_input\n","(None,)\n","*****unregularized run*****\n","skipping gradient insertion in eval mode\n","layer: fc2 with:\n","grad_output\n","(tensor([[-0.0789],\n","        [-0.0809],\n","        [-0.0831]]),)\n","grad_input\n","(tensor([[-0.0316, -0.0395],\n","        [-0.0324, -0.0404],\n","        [-0.0332, -0.0415]]),)\n","caught a gradient:\n","(tensor([[-0.0789],\n","        [-0.0809],\n","        [-0.0831]]),)\n","*****forward pass*****\n","model outputs\n","tensor([[1.0991],\n","        [1.6772],\n","        [1.0528]], grad_fn=<BackwardHookFunctionBackward>)\n","*****loss calculation*****\n","loss: 0.2527494728565216\n","*****backward pass*****\n","inserted gradient:\n","(tensor([[-0.0789],\n","        [-0.0809],\n","        [-0.0831]]),)\n","layer: fc2 with:\n","grad_output\n","(tensor([[-0.0789],\n","        [-0.0809],\n","        [-0.0831]]),)\n","grad_input\n","(tensor([[-0.0316, -0.0395],\n","        [-0.0324, -0.0404],\n","        [-0.0332, -0.0415]]),)\n","caught a gradient:\n","(tensor([[-0.0789],\n","        [-0.0809],\n","        [-0.0831]]),)\n","layer: do1 with:\n","grad_output\n","(tensor([[-0.0316, -0.0395],\n","        [-0.0324, -0.0404],\n","        [-0.0332, -0.0415]]),)\n","grad_input\n","(tensor([[-0.0632, -0.0000],\n","        [-0.0647, -0.0808],\n","        [-0.0665, -0.0000]]),)\n","layer: s1 with:\n","grad_output\n","(tensor([[-0.0632, -0.0000],\n","        [-0.0647, -0.0808],\n","        [-0.0665, -0.0000]]),)\n","grad_input\n","(tensor([[-0.0148, -0.0000],\n","        [-0.0156, -0.0194],\n","        [-0.0163, -0.0000]]),)\n","layer: fc1 with:\n","grad_output\n","(tensor([[-0.0148, -0.0000],\n","        [-0.0156, -0.0194],\n","        [-0.0163, -0.0000]]),)\n","grad_input\n","(None,)\n","*****unregularized run*****\n","skipping gradient insertion in eval mode\n","layer: fc2 with:\n","grad_output\n","(tensor([[ 0.2467],\n","        [-0.0825],\n","        [-0.0785]]),)\n","grad_input\n","(tensor([[ 0.0988,  0.1234],\n","        [-0.0330, -0.0413],\n","        [-0.0314, -0.0392]]),)\n","caught a gradient:\n","(tensor([[ 0.2467],\n","        [-0.0825],\n","        [-0.0785]]),)\n","*****forward pass*****\n","model outputs\n","tensor([[1.0782],\n","        [1.1643],\n","        [1.7563]], grad_fn=<BackwardHookFunctionBackward>)\n","*****loss calculation*****\n","loss: 0.6006711721420288\n","*****backward pass*****\n","inserted gradient:\n","(tensor([[ 0.2467],\n","        [-0.0825],\n","        [-0.0785]]),)\n","layer: fc2 with:\n","grad_output\n","(tensor([[ 0.2467],\n","        [-0.0825],\n","        [-0.0785]]),)\n","grad_input\n","(tensor([[ 0.0988,  0.1234],\n","        [-0.0330, -0.0413],\n","        [-0.0314, -0.0392]]),)\n","caught a gradient:\n","(tensor([[ 0.2467],\n","        [-0.0825],\n","        [-0.0785]]),)\n","layer: do1 with:\n","grad_output\n","(tensor([[ 0.0988,  0.1234],\n","        [-0.0330, -0.0413],\n","        [-0.0314, -0.0392]]),)\n","grad_input\n","(tensor([[ 0.0000,  0.2467],\n","        [-0.0000, -0.0825],\n","        [-0.0628, -0.0785]]),)\n","layer: s1 with:\n","grad_output\n","(tensor([[ 0.0000,  0.2467],\n","        [-0.0000, -0.0825],\n","        [-0.0628, -0.0785]]),)\n","grad_input\n","(tensor([[ 0.0000,  0.0616],\n","        [-0.0000, -0.0203],\n","        [-0.0147, -0.0178]]),)\n","layer: fc1 with:\n","grad_output\n","(tensor([[ 0.0000,  0.0616],\n","        [-0.0000, -0.0203],\n","        [-0.0147, -0.0178]]),)\n","grad_input\n","(None,)\n","*****unregularized run*****\n","skipping gradient insertion in eval mode\n","layer: fc2 with:\n","grad_output\n","(tensor([[-0.0850],\n","        [ 0.2555],\n","        [-0.0795]]),)\n","grad_input\n","(tensor([[-0.0340, -0.0425],\n","        [ 0.1023,  0.1277],\n","        [-0.0319, -0.0398]]),)\n","caught a gradient:\n","(tensor([[-0.0850],\n","        [ 0.2555],\n","        [-0.0795]]),)\n","*****forward pass*****\n","model outputs\n","tensor([[0.6001],\n","        [1.7777],\n","        [1.0928]], grad_fn=<BackwardHookFunctionBackward>)\n","*****loss calculation*****\n","loss: 0.8868257403373718\n","*****backward pass*****\n","inserted gradient:\n","(tensor([[-0.0850],\n","        [ 0.2555],\n","        [-0.0795]]),)\n","layer: fc2 with:\n","grad_output\n","(tensor([[-0.0850],\n","        [ 0.2555],\n","        [-0.0795]]),)\n","grad_input\n","(tensor([[-0.0340, -0.0425],\n","        [ 0.1023,  0.1277],\n","        [-0.0319, -0.0398]]),)\n","caught a gradient:\n","(tensor([[-0.0850],\n","        [ 0.2555],\n","        [-0.0795]]),)\n","layer: do1 with:\n","grad_output\n","(tensor([[-0.0340, -0.0425],\n","        [ 0.1023,  0.1277],\n","        [-0.0319, -0.0398]]),)\n","grad_input\n","(tensor([[-0.0000, -0.0000],\n","        [ 0.2047,  0.2555],\n","        [-0.0637, -0.0000]]),)\n","layer: s1 with:\n","grad_output\n","(tensor([[-0.0000, -0.0000],\n","        [ 0.2047,  0.2555],\n","        [-0.0637, -0.0000]]),)\n","grad_input\n","(tensor([[-0.0000, -0.0000],\n","        [ 0.0472,  0.0568],\n","        [-0.0151, -0.0000]]),)\n","layer: fc1 with:\n","grad_output\n","(tensor([[-0.0000, -0.0000],\n","        [ 0.0472,  0.0568],\n","        [-0.0151, -0.0000]]),)\n","grad_input\n","(None,)\n","train Loss: 0.5997 Acc: 0.7500\n","eval Loss: 0.5550 Acc: 0.7500\n","\n","Training complete in 0m 0s\n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                                       aten::as_strided         0.05%       8.000us         0.05%       8.000us       0.010us           840  \n","                                           aten::select         3.08%     488.000us         3.11%     493.000us       0.816us           604  \n","                                             aten::item         0.16%      25.000us         0.18%      28.000us       0.059us           476  \n","                              aten::_local_scalar_dense         0.02%       3.000us         0.02%       3.000us       0.006us           476  \n","                                               aten::to         1.30%     206.000us         5.38%     852.000us       2.266us           376  \n","                                            aten::copy_         1.84%     292.000us         1.84%     292.000us       0.973us           300  \n","                                    aten::empty_strided         0.44%      69.000us         0.44%      69.000us       0.236us           292  \n","                                            aten::empty         0.35%      55.000us         0.35%      55.000us       0.192us           287  \n","                                         aten::_to_copy         2.99%     474.000us         4.62%     732.000us       2.691us           272  \n","                                     aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us           260  \n","                                            aten::fill_         0.06%       9.000us         0.06%       9.000us       0.042us           212  \n","                                       aten::is_nonzero         0.34%      54.000us         0.35%      56.000us       0.269us           208  \n","                                              aten::abs         2.37%     375.000us         3.46%     548.000us       2.946us           186  \n","                                               aten::ne         2.23%     353.000us         4.50%     713.000us       4.571us           156  \n","                                      aten::resolve_neg         0.00%       0.000us         0.00%       0.000us       0.000us           156  \n","                                              aten::sum         2.84%     449.000us         4.61%     730.000us       6.348us           115  \n","                                               aten::gt         1.23%     195.000us         1.36%     215.000us       1.920us           112  \n","                                           aten::unbind         2.77%     438.000us         3.89%     616.000us       5.923us           104  \n","                                             aten::view         0.56%      89.000us         0.56%      89.000us       1.011us            88  \n","                                                aten::t         1.07%     170.000us         1.36%     216.000us       2.700us            80  \n","                                        aten::transpose         0.27%      43.000us         0.29%      46.000us       0.575us            80  \n","                                               aten::eq         0.49%      78.000us         0.49%      78.000us       1.300us            60  \n","                                              aten::mul         0.58%      92.000us         0.58%      92.000us       1.533us            60  \n","                                             aten::add_         0.35%      55.000us         0.35%      55.000us       0.948us            58  \n","                                   BackwardHookFunction         2.09%     331.000us         2.69%     426.000us       7.607us            56  \n","                                          aten::view_as         0.19%      30.000us         0.51%      81.000us       1.446us            56  \n","autograd::engine::evaluate_function: BackwardHookFun...        44.49%       7.045ms        69.32%      10.976ms     196.000us            56  \n","                           BackwardHookFunctionBackward         0.74%     117.000us         0.74%     117.000us       2.089us            56  \n","                                              aten::div         0.71%     112.000us         1.00%     159.000us       2.944us            54  \n","                                          aten::reshape         0.73%     115.000us         1.22%     193.000us       3.712us            52  \n","                                   aten::_reshape_alias         0.52%      82.000us         0.52%      82.000us       1.577us            52  \n","                                         aten::isfinite         1.12%     178.000us         6.45%       1.022ms      19.654us            52  \n","                                          aten::__and__         0.30%      48.000us         0.76%     121.000us       2.327us            52  \n","                                      aten::bitwise_and         0.68%     107.000us         0.68%     107.000us       2.058us            52  \n","                                    aten::masked_select         1.49%     236.000us         4.09%     648.000us      12.462us            52  \n","                                              aten::min         0.71%     113.000us         0.71%     113.000us       2.173us            52  \n","                                              aten::max         0.36%      57.000us         0.36%      57.000us       1.096us            52  \n","                                             aten::ceil         0.69%     110.000us         0.69%     110.000us       2.115us            52  \n","                                               aten::lt         0.57%      90.000us         0.57%      90.000us       1.731us            52  \n","                                           aten::detach         0.12%      19.000us         0.29%      46.000us       1.278us            36  \n","                                                 detach         0.21%      33.000us         0.21%      33.000us       0.917us            36  \n","                                              aten::neg         0.30%      48.000us         0.30%      48.000us       1.333us            36  \n","autograd::engine::evaluate_function: torch::autograd...         0.16%      26.000us         0.72%     114.000us       3.562us            32  \n","                        torch::autograd::AccumulateGrad         0.38%      60.000us         0.54%      86.000us       2.688us            32  \n","                                             aten::div_         0.59%      94.000us         1.16%     184.000us       7.360us            25  \n","                                           aten::linear         0.45%      71.000us         3.04%     481.000us      20.042us            24  \n","                                            aten::addmm         1.53%     242.000us         1.98%     313.000us      13.042us            24  \n","                                           aten::expand         0.12%      19.000us         0.12%      19.000us       0.792us            24  \n","                                       aten::empty_like         0.23%      37.000us         0.26%      41.000us       1.708us            24  \n","                                             aten::exp_         0.22%      35.000us         0.22%      35.000us       1.458us            24  \n","                                               aten::mm         0.78%     124.000us         0.78%     124.000us       5.167us            24  \n","                                          aten::sigmoid         0.59%      93.000us         0.59%      93.000us       4.650us            20  \n","                                             aten::sub_         0.25%      39.000us         0.25%      39.000us       1.950us            20  \n","                                             aten::mul_         0.23%      37.000us         0.23%      37.000us       1.850us            20  \n","                                             aten::mean         0.60%      95.000us         1.85%     293.000us      16.278us            18  \n","                                             aten::set_         0.10%      16.000us         0.10%      16.000us       1.000us            16  \n","                                            aten::stack         0.30%      48.000us         1.00%     159.000us       9.938us            16  \n","                                              aten::cat         0.49%      77.000us         0.67%     106.000us       6.625us            16  \n","    autograd::engine::evaluate_function: AddmmBackward0         0.56%      88.000us         2.92%     462.000us      28.875us            16  \n","                                         AddmmBackward0         0.51%      80.000us         1.77%     280.000us      17.500us            16  \n","        autograd::engine::evaluate_function: TBackward0         0.09%      14.000us         0.32%      50.000us       3.125us            16  \n","                                             TBackward0         0.11%      17.000us         0.21%      34.000us       2.125us            16  \n","                                          aten::dropout         0.18%      29.000us         1.09%     172.000us      14.333us            12  \n","                 aten::binary_cross_entropy_with_logits         1.24%     197.000us         4.01%     635.000us      52.917us            12  \n","                                       aten::clamp_min_         0.13%      21.000us         0.13%      21.000us       1.750us            12  \n","                                              aten::sub         0.08%      12.000us         0.08%      12.000us       1.000us            12  \n","                                             aten::log_         0.11%      18.000us         0.11%      18.000us       1.500us            12  \n","enumerate(DataLoader)#_SingleProcessDataLoaderIter._...         3.67%     581.000us         5.21%     825.000us      82.500us            10  \n","                                           aten::narrow         0.12%      19.000us         0.18%      29.000us       3.625us             8  \n","                                            aten::slice         0.06%      10.000us         0.06%      10.000us       1.250us             8  \n","                      Optimizer.zero_grad#SGD.zero_grad         0.89%     141.000us         0.89%     141.000us      17.625us             8  \n","                                        aten::ones_like         0.05%       8.000us         0.10%      16.000us       2.000us             8  \n","autograd::engine::evaluate_function: BinaryCrossEntr...         0.17%      27.000us         1.27%     201.000us      25.125us             8  \n","                  BinaryCrossEntropyWithLogitsBackward0         0.30%      48.000us         1.10%     174.000us      21.750us             8  \n","autograd::engine::evaluate_function: SigmoidBackward...         0.06%       9.000us         0.59%      94.000us      11.750us             8  \n","                                       SigmoidBackward0         0.23%      37.000us         0.54%      85.000us      10.625us             8  \n","                                 aten::sigmoid_backward         0.30%      48.000us         0.30%      48.000us       6.000us             8  \n","                                        aten::new_empty         0.04%       6.000us         0.04%       6.000us       1.500us             4  \n","                                       aten::bernoulli_         0.42%      66.000us         0.42%      66.000us      16.500us             4  \n","      autograd::engine::evaluate_function: MulBackward0         0.03%       5.000us         0.21%      33.000us       8.250us             4  \n","                                           MulBackward0         0.08%      12.000us         0.18%      28.000us       7.000us             4  \n","                                Optimizer.step#SGD.step         1.99%     315.000us         2.25%     357.000us      89.250us             4  \n","                                          aten::random_         0.09%      14.000us         0.09%      14.000us       7.000us             2  \n","                                              aten::add         0.05%       8.000us         0.05%       8.000us       4.000us             2  \n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 15.834ms\n","\n"]}],"source":["with torch.autograd.profiler.profile() as prof:\n","    hist = train_model(model,\n","                    dataloaders,\n","                    loss_fn,\n","                    optimizer,\n","                    num_epochs=1\n","                    )\n","print(prof.key_averages().table(sort_by=\"count\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QbB18j464Wme"},"source":["Compare the run with (artifical) gradient insertion (picture right side) to the one without (picture left side). It can be seen that the output gradient is being changed and propagated through the net.\n","\n","NEW PICTURE"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63,"status":"ok","timestamp":1679487416753,"user":{"displayName":"Ricupuch","userId":"12829564591003013695"},"user_tz":-60},"id":"tsdmhXg0wgRj","outputId":"46dce10d-7520-4a40-c13f-c37f2109e253"},"outputs":[{"name":"stdout","output_type":"stream","text":["OrderedDict([(3, <bound method Backward_Debug_Hook.hook_fn of <__main__.Backward_Debug_Hook object at 0x0000028118136FA0>>), (4, <bound method Backward_Debug_Hook.hook_fn of <__main__.Backward_Debug_Hook object at 0x0000028118136D00>>), (5, <bound method Catch_Hook.hook_fn of <__main__.Catch_Hook object at 0x0000028117E8F280>>)])\n","OrderedDict([(6, <bound method Insert_Hook.hook_fn of <__main__.Insert_Hook object at 0x000002811813CFD0>>)])\n","(tensor([[-0.0850],\n","        [ 0.2555],\n","        [-0.0795]]),)\n"]}],"source":["print(affected_layer._backward_hooks)\n","print(affected_layer._backward_pre_hooks)\n","print(catch_hook.caught_grad)"]},{"cell_type":"markdown","metadata":{"id":"uwDPsTtfRMXT"},"source":["# Old variant with backward_pre_hooks"]},{"cell_type":"markdown","metadata":{"id":"ImFmWLw4NZVe"},"source":["In the old variant, the catch and insert hooks were registered on the fully connected layer. For some reason, the insertion of the gradient_ouput before the computation of gradient_input was successfull, but it didn't change the gradient_input. Therefore this is here for reference purposes. Notice the use of a backward_**pre**_hook"]},{"cell_type":"code","execution_count":88,"metadata":{"id":"gTAjP10BOFep"},"outputs":[],"source":["# affected_layer = model.fc2\n","# affected_name = 'fc2'"]},{"cell_type":"code","execution_count":89,"metadata":{"id":"NpQhluLXNxHt"},"outputs":[],"source":["# class Catch_Hook_old():\n","#   def __init__(self, module):\n","#     self.hook = module.register_full_backward_hook(self.hook_fn)\n","\n","#   def hook_fn(self, module, grad_input, grad_output):\n","#     self.caught_grad = grad_output\n","#     print('caught a gradient')\n","\n","#   def close(self):\n","#     self.hook.remove()\n","\n","# catch_hook = Catch_Hook_old(affected_layer)"]},{"cell_type":"code","execution_count":90,"metadata":{"id":"lWhON4MAN_lz"},"outputs":[],"source":["# class Insert_Hook_old():\n","#   def __init__(self, module, new_grad_output=None):\n","#     self.new_grad_output = new_grad_output\n","#     # use prepend=True so that this is definetly the first hook being applied\n","#     self.hook = module.register_full_backward_pre_hook(self.hook_fn)\n","\n","#   def hook_fn(self, module, grad_output):\n","#     if module.training:\n","#       print('inserted gradient')\n","#       # simply return the previously caught grad_output\n","#       # this will replace the current grad_output (if prehook is used)\n","#       # if non-pre hook is used, grad_input will be replaced (not desire in our case)\n","#       return self.new_grad_output\n","#     else:\n","#       print('skipping gradient insertion in eval mode')\n","\n","#   def update_grad(self, new_grad_output):\n","#     self.new_grad_output = new_grad_output\n","\n","#   def close(self):\n","#     self.hook.remove()\n","\n","# insert_hook = Insert_Hook_old(affected_layer,artifical_grad)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Test question"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["without gradient insertion\n","grad_output\n","(tensor([[0.5000, 0.5000]]),)\n","grad_input\n","(tensor([[ 0.0976, -0.4175]]),)\n","with gradient insertion\n","grad_output\n","(tensor([[100., 100.]]),)\n","grad_input\n","(tensor([[ 19.5118, -83.4977]]),)\n"]}],"source":["# import torch\n","# import torch.nn as nn\n","\n","# class Backward_Debug_Hook():\n","#   def __init__(self, module):\n","#     self.hook = module.register_full_backward_hook(self.hook_fn)\n","\n","#   def hook_fn(self, module, grad_input, grad_output):\n","#     print('grad_output')\n","#     print(grad_output)\n","#     print('grad_input')\n","#     print(grad_input)\n","    \n","#   def close(self):\n","#     self.hook.remove() \n","\n","# class Insert_Hook():\n","#   def __init__(self, module, new_grad_output=None):\n","#     self.new_grad_output = new_grad_output\n","#     self.hook = module.register_full_backward_pre_hook(self.hook_fn)\n","\n","#   def hook_fn(self, module, grad_output):\n","#     return self.new_grad_output\n","\n","#   def close(self):\n","#     self.hook.remove()\n","\n","# # simple model\n","# model = nn.Sequential(\n","#   nn.Linear(2, 2),\n","#   nn.Sigmoid(),\n","#   nn.Linear(2,2)\n","# )\n","# last_layer = model[-1]\n","\n","# debug_hook = Backward_Debug_Hook(last_layer) # attach debug hook\n","# x = torch.randn(1, 2) # artificial input\n","# out = model(x) # forward pass\n","# print('without gradient insertion')\n","# out.mean().backward() # backward pass\n","\n","\n","# model.zero_grad()\n","\n","# artifical_grad = (100*torch.ones([1,2]),)\n","# insert_hook = Insert_Hook(last_layer,artifical_grad)\n","# out = model(x) # forward pass\n","# print('with gradient insertion')\n","# out.mean().backward() # backward pass"]},{"cell_type":"markdown","metadata":{"id":"3fCbPJWn58Zn"},"source":["# Test glue"]},{"cell_type":"code","execution_count":92,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1679919500034,"user":{"displayName":"Ricupuch","userId":"12829564591003013695"},"user_tz":-120},"id":"I5JSdo2X59tG","outputId":"db4bce2d-4205-40c5-e455-7c5ca3afde8a"},"outputs":[],"source":["# !python run_glue.py \\\n","#   --model_name_or_path bert-base-cased \\\n","#   --task_name RTE \\\n","#   --do_train \\\n","#   --do_eval \\\n","#   --max_seq_length 128 \\\n","#   --per_device_train_batch_size 32 \\\n","#   --learning_rate 2e-5 \\\n","#   --num_train_epochs 3 \\\n","#   --output_dir /tmp/RTE/"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Test loss grad"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1.])\n"]}],"source":["loss_fn = nn.BCELoss()\n","loss_fn2 = nn.BCEWithLogitsLoss()\n","target = torch.FloatTensor([1])\n","print(target)"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["logit tensor([0.4000], requires_grad=True)\n","sigmoid output tensor([0.5987], grad_fn=<SigmoidBackward0>)\n","loss 0.5130152702331543\n","expected loss grad tensor([-0.4013])\n","calculated gradient (tensor([-0.4013]),)\n"]}],"source":["logit = torch.tensor([0.4],requires_grad=True)\n","print(f'logit {logit}')\n","sigmoid = nn.Sigmoid()\n","proba = sigmoid(logit)\n","print(f'sigmoid output {proba}')\n","loss2 = loss_fn2(logit,target)\n","print(f'loss {loss2}')\n","\n","print(f'expected loss grad {proba.detach()-target}')\n","loss_grad = torch.autograd.grad(loss2,logit)\n","print(f'calculated gradient {loss_grad}')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["logit tensor([0.4000], requires_grad=True)\n","sigmoid output tensor([0.5987], grad_fn=<SigmoidBackward0>)\n","loss 0.5130152702331543\n","expected loss grad tensor([-0.4013])\n","calculated gradient (tensor([-2.5000]),)\n"]}],"source":["logit = torch.tensor([0.4],requires_grad=True)\n","print(f'logit {logit}')\n","sigmoid = nn.Sigmoid()\n","proba = sigmoid(logit)\n","print(f'sigmoid output {proba}')\n","loss = loss_fn(logit,target)\n","print(f'loss {loss2}')\n","\n","print(f'expected loss grad {proba.detach()-target}')\n","loss_grad = torch.autograd.grad(loss,logit)\n","print(f'calculated gradient {loss_grad}')"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["\n","x = torch.ones(5)  # input tensor\n","y = torch.zeros(3)  # expected output\n","w = torch.randn(5, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","z = torch.matmul(x, w)+b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Valentin\\AppData\\Local\\Temp\\ipykernel_22568\\4110045842.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n","  z.grad\n"]}],"source":["z.grad"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"data":{"text/plain":["['__call__',\n"," '__class__',\n"," '__delattr__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '_register_hook_dict',\n"," '_saved_alpha',\n"," 'metadata',\n"," 'name',\n"," 'next_functions',\n"," 'register_hook',\n"," 'register_prehook',\n"," 'requires_grad']"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["dir(z.grad_fn)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"data":{"image/svg+xml":["<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n","<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n"," \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n","<!-- Generated by graphviz version 2.50.0 (0)\n"," -->\n","<!-- Pages: 1 -->\n","<svg width=\"353pt\" height=\"579pt\"\n"," viewBox=\"0.00 0.00 353.00 579.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n","<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 575)\">\n","<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-575 349,-575 349,4 -4,4\"/>\n","<!-- 2214791153120 -->\n","<g id=\"node1\" class=\"node\">\n","<title>2214791153120</title>\n","<polygon fill=\"#caff70\" stroke=\"black\" points=\"139.5,-31 85.5,-31 85.5,0 139.5,0 139.5,-31\"/>\n","<text text-anchor=\"middle\" x=\"112.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n","</g>\n","<!-- 2214783902192 -->\n","<g id=\"node2\" class=\"node\">\n","<title>2214783902192</title>\n","<polygon fill=\"lightgrey\" stroke=\"black\" points=\"304,-152 65,-152 65,-67 304,-67 304,-152\"/>\n","<text text-anchor=\"middle\" x=\"184.5\" y=\"-140\" font-family=\"monospace\" font-size=\"10.00\">BinaryCrossEntropyWithLogitsBackward0</text>\n","<text text-anchor=\"middle\" x=\"184.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;</text>\n","<text text-anchor=\"middle\" x=\"184.5\" y=\"-118\" font-family=\"monospace\" font-size=\"10.00\">pos_weight: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;None</text>\n","<text text-anchor=\"middle\" x=\"184.5\" y=\"-107\" font-family=\"monospace\" font-size=\"10.00\">reduction : &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;1</text>\n","<text text-anchor=\"middle\" x=\"184.5\" y=\"-96\" font-family=\"monospace\" font-size=\"10.00\">self &#160;&#160;&#160;&#160;&#160;: [saved tensor]</text>\n","<text text-anchor=\"middle\" x=\"184.5\" y=\"-85\" font-family=\"monospace\" font-size=\"10.00\">target &#160;&#160;&#160;: [saved tensor]</text>\n","<text text-anchor=\"middle\" x=\"184.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">weight &#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;None</text>\n","</g>\n","<!-- 2214783902192&#45;&gt;2214791153120 -->\n","<g id=\"edge11\" class=\"edge\">\n","<title>2214783902192&#45;&gt;2214791153120</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M151.87,-66.81C144.47,-57.35 136.88,-47.65 130.36,-39.32\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"132.93,-36.92 124.01,-31.2 127.42,-41.24 132.93,-36.92\"/>\n","</g>\n","<!-- 2214791406352 -->\n","<g id=\"node3\" class=\"node\">\n","<title>2214791406352</title>\n","<polygon fill=\"orange\" stroke=\"black\" points=\"211.5,-30.5 157.5,-30.5 157.5,-0.5 211.5,-0.5 211.5,-30.5\"/>\n","<text text-anchor=\"middle\" x=\"184.5\" y=\"-18.5\" font-family=\"monospace\" font-size=\"10.00\">self</text>\n","<text text-anchor=\"middle\" x=\"184.5\" y=\"-7.5\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n","</g>\n","<!-- 2214783902192&#45;&gt;2214791406352 -->\n","<g id=\"edge1\" class=\"edge\">\n","<title>2214783902192&#45;&gt;2214791406352</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M184.5,-66.81C184.5,-53.88 184.5,-40.49 184.5,-30.77\"/>\n","</g>\n","<!-- 2214787884352 -->\n","<g id=\"node4\" class=\"node\">\n","<title>2214787884352</title>\n","<polygon fill=\"orange\" stroke=\"black\" points=\"283.5,-30.5 229.5,-30.5 229.5,-0.5 283.5,-0.5 283.5,-30.5\"/>\n","<text text-anchor=\"middle\" x=\"256.5\" y=\"-18.5\" font-family=\"monospace\" font-size=\"10.00\">target</text>\n","<text text-anchor=\"middle\" x=\"256.5\" y=\"-7.5\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n","</g>\n","<!-- 2214783902192&#45;&gt;2214787884352 -->\n","<g id=\"edge2\" class=\"edge\">\n","<title>2214783902192&#45;&gt;2214787884352</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M217.13,-66.81C227.25,-53.88 237.73,-40.49 245.33,-30.77\"/>\n","</g>\n","<!-- 2214783901952 -->\n","<g id=\"node5\" class=\"node\">\n","<title>2214783901952</title>\n","<polygon fill=\"lightgrey\" stroke=\"black\" points=\"229,-229 140,-229 140,-188 229,-188 229,-229\"/>\n","<text text-anchor=\"middle\" x=\"184.5\" y=\"-217\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n","<text text-anchor=\"middle\" x=\"184.5\" y=\"-206\" font-family=\"monospace\" font-size=\"10.00\">&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;</text>\n","<text text-anchor=\"middle\" x=\"184.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">alpha: 1</text>\n","</g>\n","<!-- 2214783901952&#45;&gt;2214783902192 -->\n","<g id=\"edge3\" class=\"edge\">\n","<title>2214783901952&#45;&gt;2214783902192</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M184.5,-187.87C184.5,-180.39 184.5,-171.47 184.5,-162.39\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"188,-162.13 184.5,-152.13 181,-162.13 188,-162.13\"/>\n","</g>\n","<!-- 2214783902048 -->\n","<g id=\"node6\" class=\"node\">\n","<title>2214783902048</title>\n","<polygon fill=\"lightgrey\" stroke=\"black\" points=\"149,-317 0,-317 0,-265 149,-265 149,-317\"/>\n","<text text-anchor=\"middle\" x=\"74.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward4</text>\n","<text text-anchor=\"middle\" x=\"74.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\">&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;</text>\n","<text text-anchor=\"middle\" x=\"74.5\" y=\"-283\" font-family=\"monospace\" font-size=\"10.00\">dim &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;0</text>\n","<text text-anchor=\"middle\" x=\"74.5\" y=\"-272\" font-family=\"monospace\" font-size=\"10.00\">self_sym_sizes: (1, 3)</text>\n","</g>\n","<!-- 2214783902048&#45;&gt;2214783901952 -->\n","<g id=\"edge4\" class=\"edge\">\n","<title>2214783902048&#45;&gt;2214783901952</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M108.7,-264.97C121.64,-255.51 136.37,-244.72 149.39,-235.2\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"151.65,-237.88 157.65,-229.15 147.52,-232.23 151.65,-237.88\"/>\n","</g>\n","<!-- 2214783901808 -->\n","<g id=\"node7\" class=\"node\">\n","<title>2214783901808</title>\n","<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-449 5,-449 5,-353 214,-353 214,-449\"/>\n","<text text-anchor=\"middle\" x=\"109.5\" y=\"-437\" font-family=\"monospace\" font-size=\"10.00\">MmBackward0</text>\n","<text text-anchor=\"middle\" x=\"109.5\" y=\"-426\" font-family=\"monospace\" font-size=\"10.00\">&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;</text>\n","<text text-anchor=\"middle\" x=\"109.5\" y=\"-415\" font-family=\"monospace\" font-size=\"10.00\">mat2 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;None</text>\n","<text text-anchor=\"middle\" x=\"109.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\">mat2_sym_sizes &#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(5, 3)</text>\n","<text text-anchor=\"middle\" x=\"109.5\" y=\"-393\" font-family=\"monospace\" font-size=\"10.00\">mat2_sym_strides: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(3, 1)</text>\n","<text text-anchor=\"middle\" x=\"109.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\">self &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: [saved tensor]</text>\n","<text text-anchor=\"middle\" x=\"109.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\">self_sym_sizes &#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(1, 5)</text>\n","<text text-anchor=\"middle\" x=\"109.5\" y=\"-360\" font-family=\"monospace\" font-size=\"10.00\">self_sym_strides: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;()</text>\n","</g>\n","<!-- 2214783901808&#45;&gt;2214783902048 -->\n","<g id=\"edge5\" class=\"edge\">\n","<title>2214783901808&#45;&gt;2214783902048</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M94.25,-352.94C91.42,-344.21 88.51,-335.24 85.83,-326.95\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"89.09,-325.68 82.68,-317.24 82.43,-327.84 89.09,-325.68\"/>\n","</g>\n","<!-- 2214785289488 -->\n","<g id=\"node8\" class=\"node\">\n","<title>2214785289488</title>\n","<polygon fill=\"orange\" stroke=\"black\" points=\"226,-306 167,-306 167,-276 226,-276 226,-306\"/>\n","<text text-anchor=\"middle\" x=\"196.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\">self</text>\n","<text text-anchor=\"middle\" x=\"196.5\" y=\"-283\" font-family=\"monospace\" font-size=\"10.00\"> (1, 5)</text>\n","</g>\n","<!-- 2214783901808&#45;&gt;2214785289488 -->\n","<g id=\"edge6\" class=\"edge\">\n","<title>2214783901808&#45;&gt;2214785289488</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M147.41,-352.94C160.96,-336.12 175.25,-318.38 184.9,-306.4\"/>\n","</g>\n","<!-- 2214783902912 -->\n","<g id=\"node9\" class=\"node\">\n","<title>2214783902912</title>\n","<polygon fill=\"lightgrey\" stroke=\"black\" points=\"160,-504 59,-504 59,-485 160,-485 160,-504\"/>\n","<text text-anchor=\"middle\" x=\"109.5\" y=\"-492\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n","</g>\n","<!-- 2214783902912&#45;&gt;2214783901808 -->\n","<g id=\"edge7\" class=\"edge\">\n","<title>2214783902912&#45;&gt;2214783901808</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M109.5,-484.73C109.5,-478.42 109.5,-469.31 109.5,-459.43\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"113,-459.35 109.5,-449.35 106,-459.35 113,-459.35\"/>\n","</g>\n","<!-- 2214659266496 -->\n","<g id=\"node10\" class=\"node\">\n","<title>2214659266496</title>\n","<polygon fill=\"lightblue\" stroke=\"black\" points=\"139,-571 80,-571 80,-540 139,-540 139,-571\"/>\n","<text text-anchor=\"middle\" x=\"109.5\" y=\"-547\" font-family=\"monospace\" font-size=\"10.00\"> (5, 3)</text>\n","</g>\n","<!-- 2214659266496&#45;&gt;2214783902912 -->\n","<g id=\"edge8\" class=\"edge\">\n","<title>2214659266496&#45;&gt;2214783902912</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M109.5,-539.92C109.5,-532.22 109.5,-522.69 109.5,-514.43\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"113,-514.25 109.5,-504.25 106,-514.25 113,-514.25\"/>\n","</g>\n","<!-- 2214783901760 -->\n","<g id=\"node11\" class=\"node\">\n","<title>2214783901760</title>\n","<polygon fill=\"lightgrey\" stroke=\"black\" points=\"345,-300.5 244,-300.5 244,-281.5 345,-281.5 345,-300.5\"/>\n","<text text-anchor=\"middle\" x=\"294.5\" y=\"-288.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n","</g>\n","<!-- 2214783901760&#45;&gt;2214783901952 -->\n","<g id=\"edge9\" class=\"edge\">\n","<title>2214783901760&#45;&gt;2214783901952</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M282.73,-281.39C267.76,-270.43 241.24,-251.02 219.53,-235.13\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"221.31,-232.1 211.17,-229.02 217.17,-237.75 221.31,-232.1\"/>\n","</g>\n","<!-- 2214659266656 -->\n","<g id=\"node12\" class=\"node\">\n","<title>2214659266656</title>\n","<polygon fill=\"lightblue\" stroke=\"black\" points=\"321.5,-416.5 267.5,-416.5 267.5,-385.5 321.5,-385.5 321.5,-416.5\"/>\n","<text text-anchor=\"middle\" x=\"294.5\" y=\"-392.5\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n","</g>\n","<!-- 2214659266656&#45;&gt;2214783901760 -->\n","<g id=\"edge10\" class=\"edge\">\n","<title>2214659266656&#45;&gt;2214783901760</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M294.5,-385.43C294.5,-366.11 294.5,-331.96 294.5,-310.74\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"298,-310.7 294.5,-300.7 291,-310.7 298,-310.7\"/>\n","</g>\n","</g>\n","</svg>\n"],"text/plain":["<graphviz.graphs.Digraph at 0x203ab6862e0>"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["from torchviz import make_dot\n","make_dot(loss,show_attrs=True,show_saved=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNm4NfwLszHXT+f9TmUVAfo","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"fantastic-umbrella","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"872823796615312064415006207907700bb7d7dbd45c19994fdc061837d1206a"}}},"nbformat":4,"nbformat_minor":0}

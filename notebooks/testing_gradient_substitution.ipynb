{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, hidden_size=2):\n",
    "        super().__init__()\n",
    "        self.last_layer = nn.Linear(2, 2)\n",
    "        self.rll = nn.ReLU()\n",
    "        self.classifier = nn.Linear(2, 2)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.last_layer(x)\n",
    "        x = self.rll(x)\n",
    "        x = self.classifier(x)\n",
    "        x = self.softmax(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1070, -0.3308], grad_fn=<ViewBackward0>)\n",
      "tensor([0.1070, 0.0000], grad_fn=<ReluBackward0>)\n",
      "tensor([-0.2152, -0.1457], grad_fn=<ViewBackward0>)\n",
      "tensor([-0.2152, -0.1457], grad_fn=<BackwardHookFunctionBackward>)\n",
      "(tensor([ 0.4826, -0.4826]),)\n",
      "Linear(in_features=2, out_features=2, bias=True) (tensor([ 0.4826, -0.4826]),)\n",
      "Linear(in_features=2, out_features=2, bias=True) (tensor([0.0235, 0.1803]),) (tensor([ 0.4826, -0.4826]),)\n",
      "Linear(in_features=2, out_features=2, bias=True) (None,) (tensor([0.0235, 0.0000]),)\n",
      "last_layer.weight tensor([[0.0000, 0.0235],\n",
      "        [0.0000, 0.0000]])\n",
      "last_layer.bias tensor([0.0235, 0.0000])\n",
      "classifier.weight tensor([[ 0.0516,  0.0000],\n",
      "        [-0.0516, -0.0000]])\n",
      "classifier.bias tensor([ 0.4826, -0.4826])\n"
     ]
    }
   ],
   "source": [
    "input =torch.tensor([0.0,1.0])\n",
    "label = torch.tensor(1)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "model = LinearNet()\n",
    "for n,m in model.named_modules():\n",
    "    m.register_forward_hook(lambda m,i,o: print(o))\n",
    "\n",
    "model.classifier.register_full_backward_pre_hook(lambda m,o: print(m,o))\n",
    "def break_graph(module, grad_output):\n",
    "    print(grad_output)\n",
    "model.classifier.register_full_backward_pre_hook(break_graph,prepend=True)\n",
    "model.classifier.register_full_backward_hook(lambda m,i,o: print(m,i,o))\n",
    "\n",
    "model.last_layer.register_full_backward_hook(lambda m,i,o: print(m,i,o))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "logits = model(input)\n",
    "\n",
    "# print(logits)\n",
    "loss = criterion(logits, label)\n",
    "loss.backward(retain_graph=True)\n",
    "# print(loss)\n",
    "\n",
    "\n",
    "for n,p in model.named_parameters():\n",
    "    print(n,p.grad)\n",
    "    # print(n,p.grad_fn)\n",
    "    # print(n,p.grad_fn.next_functions)\n",
    "    # print(n,p.grad_fn.next_functions[0][0].next_functions)\n",
    "    # print(n,p.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "    # print(n,p.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "    # print(n,p.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Layers\n",
      "[('last_layer.bias', Parameter containing:\n",
      "tensor([-0.2723,  0.1896], requires_grad=True)), ('classifier.weight', Parameter containing:\n",
      "tensor([[-0.0140,  0.5607],\n",
      "        [-0.0628,  0.1871]], requires_grad=True)), ('classifier.bias', Parameter containing:\n",
      "tensor([-0.2137, -0.1390], requires_grad=True))]\n",
      "(tensor([ 0.4826, -0.4826]),)\n",
      "Linear(in_features=2, out_features=2, bias=True) (tensor([ 0.4826, -0.4826]),)\n",
      "Linear(in_features=2, out_features=2, bias=True) (tensor([0.0235, 0.1803]),) (tensor([ 0.4826, -0.4826]),)\n",
      "Linear(in_features=2, out_features=2, bias=True) (None,) (tensor([0.0235, 0.0000]),)\n",
      "Grad Top\n",
      "(tensor([0.0235, 0.0000]), tensor([[ 0.0516,  0.0000],\n",
      "        [-0.0516, -0.0000]]), tensor([ 0.4826, -0.4826]))\n",
      "Gradients\n",
      "last_layer.weight None\n",
      "last_layer.bias None\n",
      "classifier.weight None\n",
      "classifier.bias None\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "relevant_layers = [np for np in model.named_parameters()][1:]\n",
    "print(\"Relevant Layers\")\n",
    "print(relevant_layers)\n",
    "grad_top = torch.autograd.grad(loss, [np[1] for np in relevant_layers], retain_graph=True)\n",
    "print(\"Grad Top\")\n",
    "print(grad_top)\n",
    "print(\"Gradients\")\n",
    "for n,p in model.named_parameters():\n",
    "    print(n,p.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass from gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2795, 0.4243]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a 2-dimensional tensor.\n",
    "x = torch.randn(1, 2, requires_grad=True)\n",
    "\n",
    "# Define a linear layer.\n",
    "linear = nn.Linear(2, 2)\n",
    "\n",
    "# Apply the linear layer to x to get y.\n",
    "y = linear(x)\n",
    "\n",
    "# Apply softmax to y to get z.\n",
    "softmax = nn.Softmax(dim=1)\n",
    "z = softmax(y)\n",
    "\n",
    "# Manually calculate dz/dy.\n",
    "dz_dy = torch.tensor([[1.0, 0.0]])\n",
    "\n",
    "# Call backward on y, passing in the manually calculated gradient.\n",
    "y.backward(dz_dy)\n",
    "\n",
    "# Print dx/dy.\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamically starting the DCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.1070, 0.0000], requires_grad=True),)\n",
      "(tensor([0.0120, 0.0917]),)\n",
      "last_layer.weight None\n",
      "last_layer.bias None\n",
      "classifier.weight tensor([[ 0.0262,  0.0000],\n",
      "        [-0.0262, -0.0000]])\n",
      "classifier.bias tensor([ 0.2454, -0.2454])\n"
     ]
    }
   ],
   "source": [
    "input =torch.tensor([0.0,1.0])\n",
    "label = torch.tensor(1)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "model = LinearNet()\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for p in model.classifier.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "def start_dcg(module, input):\n",
    "    for tensor in input:\n",
    "        tensor.requires_grad = True\n",
    "    return input\n",
    "\n",
    "def debug_forward(module, input):\n",
    "    print(input)\n",
    "\n",
    "def debug_backward(module, grad_input, grad_output):\n",
    "    print(grad_input)\n",
    "\n",
    "model.classifier.register_forward_pre_hook(start_dcg)\n",
    "model.classifier.register_forward_pre_hook(debug_forward)\n",
    "model.classifier.register_full_backward_hook(debug_backward)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "logits = model(input)\n",
    "\n",
    "loss = criterion(logits, label)\n",
    "loss.backward()\n",
    "\n",
    "for n,p in model.named_parameters():\n",
    "    print(n,p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the gradient calculation in the middle of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.1070, 0.0000], grad_fn=<ReluBackward0>),)\n",
      "Input grad\n",
      "(tensor([0.0120, 0.0917]),)\n",
      "last_layer.weight tensor([[0.0000, 0.0120],\n",
      "        [0.0000, 0.0000]])\n",
      "last_layer.bias tensor([0.0120, 0.0000])\n",
      "classifier.weight None\n",
      "classifier.bias None\n"
     ]
    }
   ],
   "source": [
    "input =torch.tensor([0.0,1.0])\n",
    "label = torch.tensor(1)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "model = LinearNet()\n",
    "\n",
    "for p in model.classifier.parameters():\n",
    "    p.requires_grad = False\n",
    "class Catch_Hook():\n",
    "    def __init__(self, module):\n",
    "        self.hook = module.register_forward_pre_hook(self.hook_fn)\n",
    "        self.bhook = module.register_full_backward_hook(self.bhook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input):\n",
    "        self.caught_tensor = input\n",
    "\n",
    "    def bhook_fn(self, module, grad_input, grad_output):\n",
    "        print('Input grad')\n",
    "        print(grad_input)\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "hook = Catch_Hook(model.classifier)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "logits = model(input)\n",
    "print(hook.caught_tensor)\n",
    "loss = criterion(logits, label)\n",
    "\n",
    "starting_grad = torch.Tensor([0.0120, 0.0917])\n",
    "# hook.caught_tensor[0].backward(starting_grad)\n",
    "loss.backward()\n",
    "\n",
    "for n,p in model.named_parameters():\n",
    "    print(n,p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('last_layer.weight', Parameter containing:\n",
      "tensor([[-0.0053,  0.3793],\n",
      "        [-0.5820, -0.5204]], requires_grad=True))\n",
      "('last_layer.bias', Parameter containing:\n",
      "tensor([-0.2723,  0.1896], requires_grad=True))\n",
      "('classifier.weight', Parameter containing:\n",
      "tensor([[-0.0140,  0.5607],\n",
      "        [-0.0628,  0.1871]]))\n",
      "('classifier.bias', Parameter containing:\n",
      "tensor([-0.2137, -0.1390]))\n",
      "++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not reversible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m      5\u001b[0m set_status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m==\u001b[39m m:\n\u001b[0;32m      8\u001b[0m         set_status \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m set_status\n",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not reversible"
     ]
    }
   ],
   "source": [
    "for np in model.named_parameters():\n",
    "    print(np)\n",
    "\n",
    "print('+'*40)\n",
    "set_status = True\n",
    "for m in reversed(model.modules()):\n",
    "    if model.classifier == m:\n",
    "        set_status = not set_status\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = set_status\n",
    "\n",
    "for np in model.named_parameters():\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also ich brauche:\n",
    "\n",
    "1. ich w√§hle eine Layer\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fumbrella():\n",
    "    def __init__(\n",
    "            self,\n",
    "            module,\n",
    "            all_modules,\n",
    "            batch_size,\n",
    "            position : str = 'input',\n",
    "            stage = 1\n",
    "            ):\n",
    "        # position: 'input', 'output' \n",
    "        self.position = position\n",
    "        self.all_modules = all_modules\n",
    "        self.module = module\n",
    "        if position == 'input':\n",
    "            self.fhook = module.register_forward_pre_hook(self.forward_pre_hook_fn)\n",
    "            self.bhook = module.register_full_backward_hook(self.backward_hook_fn)\n",
    "        elif position == 'output':\n",
    "            self.fhook = module.register_forward_hook(self.forward_hook_fn)\n",
    "            self.bhook = module.register_full_backward_pre_hook(self.backward_pre_hook_fn)\n",
    "\n",
    "        self.stage1_grad = None\n",
    "        self.set_stage(stage)\n",
    "\n",
    "        # metrics\n",
    "        self.vector_norms = []\n",
    "        self.rescaled_diffs = []\n",
    "        self.avg_diff_per_class = []\n",
    "        self.avg_diff_all_classes = []\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward_pre_hook_fn(self, module, input):\n",
    "        # stage 1\n",
    "        # need to activate the gradient calculation\n",
    "        if self.stage == 1:\n",
    "            for tensor in input:\n",
    "                tensor.requires_grad = True\n",
    "            return input\n",
    "        \n",
    "    def forward_hook_fn(self, module, input, output):\n",
    "        # stage 1\n",
    "        # need to activate the gradient calculation\n",
    "        if self.stage == 1:\n",
    "            for tensor in output:\n",
    "                tensor.requires_grad = True\n",
    "            return output\n",
    "\n",
    "    def backward_hook_fn(self, module, grad_input, grad_output):\n",
    "        # stage 1\n",
    "        if self.stage == 1:\n",
    "            self.stage1_grad = grad_input\n",
    "        # stage 2\n",
    "        if self.stage == 2:\n",
    "            self.add_metrics(self.stage1_grad, grad_input)\n",
    "            return self.stage1_grad\n",
    "            \n",
    "        \n",
    "    def backward_pre_hook_fn(self, module, grad_input, grad_output):\n",
    "        # stage 1\n",
    "        if self.stage == 1:\n",
    "            self.stage1_grad = grad_output\n",
    "        # stage 2\n",
    "        if self.stage == 2:\n",
    "            self.add_metrics(self.stage1_grad, grad_output)\n",
    "            return self.stage1_grad\n",
    "\n",
    "    def set_stage(self, stage):\n",
    "        self.stage = stage\n",
    "        if self.position == 'input':\n",
    "            direction = 'ascending'\n",
    "        elif self.position == 'output':\n",
    "            direction = 'descending'\n",
    "        if stage == 1:\n",
    "            state = False\n",
    "        if stage == 2:\n",
    "            state = True\n",
    "        self.set_requires_grad(state, direction)\n",
    "\n",
    "    def set_requires_grad(self, state : bool,direction : str):\n",
    "        # direction: 'ascending', 'descending'\n",
    "        module_iterator = self.all_modules\n",
    "        if direction == 'descending':\n",
    "            module_iterator = reversed(module_iterator)\n",
    "        for m in module_iterator:\n",
    "            if self.module == m:\n",
    "                state = not state\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = state\n",
    "\n",
    "    def add_metrics(self,stage1_grad, stage2_grad):\n",
    "        grad_diff = stage2_grad[0] - stage1_grad[0]\n",
    "        grad_diff_rescaled = grad_diff *self.batch_size\n",
    "        self.rescaled_diffs.append(grad_diff_rescaled)\n",
    "        self.vector_norms.append(torch.linalg.vector_norm(grad_diff_rescaled,dim=1))\n",
    "        self.avg_diff_per_class.append(grad_diff_rescaled.abs().mean(dim=0))\n",
    "        self.avg_diff_all_classes.append(self.avg_diff_per_class[-1].mean())\n",
    "\n",
    "        \n",
    "    def compute_diff_metrics(self,accelerator):\n",
    "        diff_metrics = {}\n",
    "        vector_norms = accelerator.gather_for_metrics(self.vector_norms)\n",
    "        if isinstance(vector_norms, list):\n",
    "            vector_norms = torch.cat(vector_norms)\n",
    "        avg_grad_diffs_per_class = accelerator.gather_for_metrics(self.avg_diff_per_class)\n",
    "        if isinstance(avg_grad_diffs_per_class, list):\n",
    "            avg_grad_diffs_per_class = torch.stack(avg_grad_diffs_per_class).mean(dim=0)\n",
    "        avg_grad_diffs_all_classes = accelerator.gather_for_metrics(self.avg_diff_all_classes)\n",
    "        if isinstance(avg_grad_diffs_all_classes, list):\n",
    "            avg_grad_diffs_all_classes = torch.stack(avg_grad_diffs_all_classes).mean()\n",
    "        self.clear_metrics()\n",
    "        diff_metrics = {\n",
    "            \"avg_grad_diff_all_classes\" : avg_grad_diffs_all_classes,\n",
    "            \"avg_grad_diff_per_class\" : avg_grad_diffs_per_class,\n",
    "            \"vector_norms\" : vector_norms\n",
    "        }\n",
    "        return diff_metrics\n",
    "    \n",
    "    def clear_metrics(self):\n",
    "        self.vector_norms = []\n",
    "        self.rescaled_diffs = []\n",
    "        self.avg_diff_per_class = []\n",
    "        self.avg_diff_all_classes = []\n",
    "\n",
    "    def close(self):\n",
    "        self.fhook.remove()\n",
    "        self.bhook.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fantastic-umbrella_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

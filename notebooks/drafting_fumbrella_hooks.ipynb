{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class FumbrellaMetrics():\n",
    "    def __init__(self, batch_size):\n",
    "        self.vector_norms = []\n",
    "        self.rescaled_diffs = []\n",
    "        self.avg_diff_per_class = []\n",
    "        self.avg_diff_all_classes = []\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, stage1_grad, stage2_grad):\n",
    "        grad_diff = stage2_grad[0] - stage1_grad[0]\n",
    "        grad_diff_rescaled = grad_diff * self.batch_size\n",
    "        self.rescaled_diffs.append(grad_diff_rescaled)\n",
    "        self.vector_norms.append(torch.linalg.vector_norm(grad_diff_rescaled,dim=1))\n",
    "        self.avg_diff_per_class.append(grad_diff_rescaled.abs().mean(dim=0))\n",
    "        self.avg_diff_all_classes.append(self.avg_diff_per_class[-1].mean())\n",
    "\n",
    "    def compute(self, accelerator):\n",
    "        diff_metrics = {}\n",
    "        vector_norms = accelerator.gather_for_metrics(self.vector_norms)\n",
    "        if isinstance(vector_norms, list):\n",
    "            vector_norms = torch.cat(vector_norms)\n",
    "        avg_grad_diffs_per_class = accelerator.gather_for_metrics(self.avg_diff_per_class)\n",
    "        if isinstance(avg_grad_diffs_per_class, list):\n",
    "            avg_grad_diffs_per_class = torch.stack(avg_grad_diffs_per_class).mean(dim=0)\n",
    "        avg_grad_diffs_all_classes = accelerator.gather_for_metrics(self.avg_diff_all_classes)\n",
    "        if isinstance(avg_grad_diffs_all_classes, list):\n",
    "            avg_grad_diffs_all_classes = torch.stack(avg_grad_diffs_all_classes).mean()\n",
    "        self.clear()\n",
    "        diff_metrics = {\n",
    "            \"avg_grad_diff_all_classes\" : avg_grad_diffs_all_classes,\n",
    "            \"avg_grad_diff_per_class\" : avg_grad_diffs_per_class,\n",
    "            \"vector_norms\" : vector_norms\n",
    "        }\n",
    "        return diff_metrics\n",
    "\n",
    "    def clear(self):\n",
    "        self.vector_norms = []\n",
    "        self.rescaled_diffs = []\n",
    "        self.avg_diff_per_class = []\n",
    "        self.avg_diff_all_classes = []\n",
    "\n",
    "\n",
    "class Fumbrella:\n",
    "    def __init__(\n",
    "            self,\n",
    "            module,\n",
    "            model,\n",
    "            batch_size,\n",
    "            stage1_dropout : float,\n",
    "            stage2_dropout : float,\n",
    "            position : str = 'input',\n",
    "            stage = 1\n",
    "            ):\n",
    "        # position: 'input', 'output' \n",
    "        self.position = position\n",
    "        self.module = module\n",
    "        self._register_hooks()\n",
    "        self._prepare_module_lists(model)\n",
    "        self.dropout_rates = {\n",
    "            1 : stage1_dropout,\n",
    "            2 : stage2_dropout\n",
    "        }\n",
    "        self.set_stage(stage)\n",
    "        self.stage1_grad = None\n",
    "        # metrics\n",
    "        self.metrics = FumbrellaMetrics(batch_size)\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        if self.position == 'input':\n",
    "            self.fhook = self.module.register_forward_pre_hook(self._forward_pre_hook_fn)\n",
    "            self.bhook = self.module.register_full_backward_hook(self._backward_hook_fn)\n",
    "        elif self.position == 'output':\n",
    "            self.fhook = self.module.register_forward_hook(self._forward_hook_fn)\n",
    "            self.bhook = self.module.register_full_backward_pre_hook(self._backward_pre_hook_fn)\n",
    "\n",
    "    def _forward_pre_hook_fn(self, module, input):\n",
    "        # stage 1\n",
    "        # need to activate the gradient calculation\n",
    "        if self.stage == 1:\n",
    "            for tensor in input:\n",
    "                tensor.requires_grad = True\n",
    "            return input\n",
    "        \n",
    "    def _forward_hook_fn(self, module, input, output):\n",
    "        # stage 1\n",
    "        # need to activate the gradient calculation\n",
    "        if self.stage == 1:\n",
    "            for tensor in output:\n",
    "                tensor.requires_grad = True\n",
    "            return output\n",
    "\n",
    "    def _backward_hook_fn(self, module, grad_input, grad_output):\n",
    "        # stage 1\n",
    "        if self.stage == 1:\n",
    "            self.stage1_grad = grad_input\n",
    "        # stage 2\n",
    "        if self.stage == 2:\n",
    "            self.metrics.add(self.stage1_grad, grad_input)\n",
    "            return self.stage1_grad\n",
    "              \n",
    "    def _backward_pre_hook_fn(self, module, grad_input, grad_output):\n",
    "        # stage 1\n",
    "        if self.stage == 1:\n",
    "            self.stage1_grad = grad_output\n",
    "        # stage 2\n",
    "        if self.stage == 2:\n",
    "            self.metrics.add(self.stage1_grad, grad_output)\n",
    "            return self.stage1_grad\n",
    "        \n",
    "    def _prepare_module_lists(self, model):\n",
    "        self.dropout_modules = [m for m in model.modules() if isinstance(m, torch.nn.Dropout)]\n",
    "        self.all_parameters = [p for p in model.parameters()]\n",
    "        if self.position == 'input':\n",
    "            self.stage1_parameters = list(self.module.parameters())\n",
    "        elif self.position == 'output':\n",
    "            self.stage1_parameters = []\n",
    "\n",
    "\n",
    "    def set_stage(self, stage: int):\n",
    "        self.stage = stage\n",
    "        for m in self.dropout_modules:\n",
    "            m.p = self.dropout_rates[stage]\n",
    "\n",
    "        for p in self.all_parameters:\n",
    "            p.requires_grad = stage == 2\n",
    "        for p in self.stage1_parameters:\n",
    "            p.requires_grad = stage == 1\n",
    "\n",
    "    def compute_diff_metrics(self,accelerator):\n",
    "        return self.metrics.compute(accelerator)\n",
    "\n",
    "    def close(self):\n",
    "        self.fhook.remove()\n",
    "        self.bhook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fantastic-umbrella_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

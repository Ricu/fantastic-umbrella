--- run_glue_no_trainer_ORIGINAL.py	2024-01-19 15:53:10.976000000 +0100
+++ run_glue_no_trainer_modded_FINAL.py	2024-01-19 15:49:48.684000000 +0100
@@ -18,20 +18,22 @@
 import logging
 import math
 import os
-import random
-from pathlib import Path
+import numpy as np
 
 import datasets
 import evaluate
 import torch
+
 from accelerate import Accelerator
 from accelerate.logging import get_logger
 from accelerate.utils import set_seed
-from datasets import load_dataset
-from huggingface_hub import Repository, create_repo
-from torch.utils.data import DataLoader
+from datasets import load_dataset, concatenate_datasets
+from torch.utils.data import DataLoader, Subset, ConcatDataset
 from tqdm.auto import tqdm
 
+from sklearn.model_selection import train_test_split
+from datetime import datetime
+
 import transformers
 from transformers import (
     AutoConfig,
@@ -46,7 +48,6 @@
 from transformers.utils import check_min_version, get_full_repo_name, send_example_telemetry
 from transformers.utils.versions import require_version
 
-
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.31.0.dev0")
 
@@ -103,11 +104,6 @@
         required=True,
     )
     parser.add_argument(
-        "--use_slow_tokenizer",
-        action="store_true",
-        help="If passed, will use a slow tokenizer (not backed by the ðŸ¤— Tokenizers library).",
-    )
-    parser.add_argument(
         "--per_device_train_batch_size",
         type=int,
         default=8,
@@ -134,12 +130,6 @@
         help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
     )
     parser.add_argument(
-        "--gradient_accumulation_steps",
-        type=int,
-        default=1,
-        help="Number of updates steps to accumulate before performing a backward/update pass.",
-    )
-    parser.add_argument(
         "--lr_scheduler_type",
         type=SchedulerType,
         default="linear",
@@ -149,24 +139,19 @@
     parser.add_argument(
         "--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
     )
-    parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
-    parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
-    parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
     parser.add_argument(
-        "--hub_model_id", type=str, help="The name of the repository to keep in sync with the local `output_dir`."
-    )
-    parser.add_argument("--hub_token", type=str, help="The token to use to push to the Model Hub.")
-    parser.add_argument(
-        "--checkpointing_steps",
-        type=str,
-        default=None,
-        help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.",
+        "--warmup_steps_fraction",
+        type=float,
+        default=None, 
+        help="Fraction of warmupsteps of total steps. Overrides `num_warmup_steps`"
     )
+    parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
+    parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
+    parser.add_argument("--dataset_seed", type=int, default=None, help="A seed for reproducible datasets.")
     parser.add_argument(
-        "--resume_from_checkpoint",
-        type=str,
-        default=None,
-        help="If the training should continue from a checkpoint folder.",
+        "--no_checkpointing",
+        action="store_true",
+        help="Whether to enable model checkpointing.",
     )
     parser.add_argument(
         "--with_tracking",
@@ -188,6 +173,67 @@
         action="store_true",
         help="Whether or not to enable to load a pretrained model whose head dimensions are different.",
     )
+    parser.add_argument(
+        "--insert_dropout",
+        type=float,
+        default=-1,
+        help="Change the dropout rate of the hidden layers during insertion.",
+    )
+    parser.add_argument(
+        "--training_size",
+        type=float,
+        default=1,
+        help="Change the fraction of training data used.",
+    )
+    parser.add_argument(
+        "--beta1",
+        type=float,
+        default=0.9,
+        help="Change the beta1 value of the AdamW optimizer.",
+    )
+    parser.add_argument(
+        "--beta2",
+        type=float,
+        default=0.999,
+        help="Change the beta2 value of the AdamW optimizer.",
+    )
+    parser.add_argument(
+        "--catch_dropout",
+        type=float,
+        default=None,
+        help="Change the dropout rate when catching a gradient. Not giving a value results in using the dropout value from the pretraining.",
+    )
+    parser.add_argument(
+        "--early_stopping_patience",
+        type=int,
+        default=10,
+        help="Set the number early stopping patience."
+    )
+    parser.add_argument(
+        "--early_stopping_min_delta",
+        type=float,
+        default=0,
+        help="Set the minimum loss delta for the early stopping."
+    )
+    parser.add_argument(
+        "--param_config_id",
+        type=str,
+        default=None,
+        help="Distinct ID by which the constellation of hyperparameters can be identified"
+    )
+    parser.add_argument(
+        "--run_generation",
+        type=str,
+        default=None,
+        help="The distinct generation from which the run comes"
+    )
+    parser.add_argument(
+        "--evaluation_steps",
+        type=int,
+        default=None,
+        help="Number of steps until one evaluation is performed."
+    )
+
     args = parser.parse_args()
 
     # Sanity checks
@@ -201,17 +247,269 @@
             extension = args.validation_file.split(".")[-1]
             assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."
 
-    if args.push_to_hub:
-        assert args.output_dir is not None, "Need an `output_dir` to create a repo when `--push_to_hub` is passed."
-
     return args
 
 
+class FumbrellaMetrics():
+    def __init__(self, batch_size):
+        self.vector_norms = []
+        self.rescaled_diffs = []
+        self.avg_diff_per_class = []
+        self.avg_diff_all_classes = []
+        self.batch_size = batch_size
+
+    def add(self, stage1_grad, stage2_grad):
+        grad_diff = stage2_grad[0] - stage1_grad[0]
+        grad_diff_rescaled = grad_diff * self.batch_size
+        self.rescaled_diffs.append(grad_diff_rescaled)
+        self.vector_norms.append(torch.linalg.vector_norm(grad_diff_rescaled,dim=1))
+        self.avg_diff_per_class.append(grad_diff_rescaled.abs().mean(dim=0))
+        self.avg_diff_all_classes.append(self.avg_diff_per_class[-1].mean())
+
+    def compute(self, accelerator):
+        diff_metrics = {}
+        vector_norms = accelerator.gather_for_metrics(self.vector_norms)
+        if isinstance(vector_norms, list):
+            vector_norms = torch.cat(vector_norms)
+        avg_grad_diffs_per_class = accelerator.gather_for_metrics(self.avg_diff_per_class)
+        if isinstance(avg_grad_diffs_per_class, list):
+            avg_grad_diffs_per_class = torch.stack(avg_grad_diffs_per_class).mean(dim=0)
+        avg_grad_diffs_all_classes = accelerator.gather_for_metrics(self.avg_diff_all_classes)
+        if isinstance(avg_grad_diffs_all_classes, list):
+            avg_grad_diffs_all_classes = torch.stack(avg_grad_diffs_all_classes).mean()
+        self.clear()
+        diff_metrics = {
+            "avg_grad_diff_all_classes" : avg_grad_diffs_all_classes,
+            "avg_grad_diff_per_class" : avg_grad_diffs_per_class,
+            "vector_norms" : vector_norms
+        }
+        return diff_metrics
+
+    def clear(self):
+        self.vector_norms = []
+        self.rescaled_diffs = []
+        self.avg_diff_per_class = []
+        self.avg_diff_all_classes = []
+
+
+class Fumbrella:
+    def __init__(
+            self,
+            module,
+            model,
+            batch_size,
+            stage1_dropout : float,
+            stage2_dropout : float,
+            position : str = 'output',
+            stage = 1
+            ):
+        # position: 'input', 'output' 
+        self.position = position
+        self.module = module
+        self._register_hooks()
+        self._prepare_module_lists(model)
+        self.dropout_rates = {
+            1 : stage1_dropout,
+            2 : stage2_dropout
+        }
+        self.set_stage(stage)
+        self.stage1_grad = None
+        # metrics
+        self.metrics = FumbrellaMetrics(batch_size)
+
+    def _register_hooks(self):
+        if self.position == 'input':
+            self.fhook = self.module.register_forward_pre_hook(self._forward_pre_hook_fn)
+            self.bhook = self.module.register_full_backward_hook(self._backward_hook_fn)
+        elif self.position == 'output':
+            self.fhook = self.module.register_forward_hook(self._forward_hook_fn)
+            self.bhook = self.module.register_full_backward_pre_hook(self._backward_pre_hook_fn)
+
+    def _forward_pre_hook_fn(self, module, input):
+        # stage 1
+        # need to activate the gradient calculation
+        if self.stage == 1:
+            for tensor in input:
+                tensor.requires_grad = True
+            return input
+        
+    def _forward_hook_fn(self, module, input, output):
+        # stage 1
+        # need to activate the gradient calculation
+        if self.stage == 1:
+            output.requires_grad = True
+            return output
+
+    def _backward_hook_fn(self, module, grad_input, grad_output):
+        # stage 1
+        if self.stage == 1:
+            self.stage1_grad = grad_input
+        # stage 2
+        if self.stage == 2:
+            self.metrics.add(self.stage1_grad, grad_input)
+            return self.stage1_grad
+              
+    def _backward_pre_hook_fn(self, module, grad_output):
+        # stage 1
+        if self.stage == 1:
+            self.stage1_grad = grad_output
+        # stage 2
+        if self.stage == 2:
+            self.metrics.add(self.stage1_grad, grad_output)
+            return self.stage1_grad
+        
+    def _prepare_module_lists(self, model):
+        self.dropout_modules = [m for m in model.modules() if isinstance(m, torch.nn.Dropout)]
+        self.all_parameters = [p for p in model.parameters()]
+        if self.position == 'input':
+            self.stage1_parameters = list(self.module.parameters())
+        elif self.position == 'output':
+            self.stage1_parameters = []
+
+    def set_stage(self, stage: int):
+        self.stage = stage
+        for m in self.dropout_modules:
+            m.p = self.dropout_rates[stage]
+
+        for p in self.all_parameters:
+            p.requires_grad = stage == 2
+        for p in self.stage1_parameters:
+            p.requires_grad = stage == 1
+
+    def compute_diff_metrics(self,accelerator):
+        return self.metrics.compute(accelerator)
+
+    def close(self):
+        self.fhook.remove()
+        self.bhook.remove()
+
+
+class EarlyStoppingCallback:
+  def __init__(self,metric_name,direction='min',min_delta=0,patience=5):
+    '''
+    `direction` is either 'min' or 'max' 
+    '''
+    self.metric_name=metric_name
+    if direction not in {'max', 'min'}:
+        raise ValueError(
+            f'Invalid direction: {direction}. Should be one of: max, min'
+        )
+    self.direction = direction
+    self.sign = 1 if direction == 'min' else -1
+    self.min_delta=min_delta
+    self.patience=patience
+    self.wait_steps=0
+    self.improved_metric= False
+    self.best_value=self.sign * math.inf
+    self.best_epoch=0
+    self.best_step=0
+
+  def check_early_stopping(self,value,epoch,step):
+    delta = self.sign * (self.best_value - value)
+    if delta >= self.min_delta: # improved metric
+        self.best_value = value
+        self.best_epoch = epoch
+        self.best_step = step
+        self.wait_steps = 0
+        self.improved_metric = True        
+    else: # not improved metric
+        self.wait_steps += 1
+        self.improved_metric = False
+    return self.wait_steps >= self.patience
+            
+    return False
+
+
+class TrainStatsHelper():
+    def __init__(self,evaluation_steps : int):
+        self.initialize_stats()
+        self.evaluation_steps = evaluation_steps
+
+    def initialize_stats(self):
+        self.stats_dict = {}
+    
+    def add_stats(self,stats : dict):
+        for stats_name, stats_value in stats.items():
+            if stats_name in self.stats_dict:
+                self.stats_dict[stats_name] += stats_value
+            else:
+                self.stats_dict[stats_name] = stats_value
+
+    def compute_stats(self, avg_factor = None, accelerator = None):
+        if avg_factor is None:
+            avg_factor = self.evaluation_steps
+
+        for stats_name, stats_value in self.stats_dict.items():
+            self.stats_dict[stats_name] = stats_value / avg_factor
+
+        if accelerator is not None and accelerator.num_processes > 1:
+            for stats_name, stats_value in self.stats_dict.items():
+                self.stats_dict[stats_name] = accelerator.gather(stats_value).mean()
+
+        return self.stats_dict
+
+
+    
+def evaluate_model(
+        model,
+        eval_dataloader,
+        accelerator,
+        metric,
+        softmax_fn,
+        is_regression,
+    ):
+
+    validation_stats = {
+        "eval_loss" : 0,
+        "eval_p_max": 0,
+        "eval_p_var" : 0
+    }
+    samples_seen = 0
+    model.eval()
+    for step, batch in enumerate(eval_dataloader):
+        with torch.no_grad():
+            outputs = model(**batch)
+            validation_stats["eval_loss"] += outputs.loss.detach().float()
+            validation_stats["eval_p_max"] += softmax_fn(outputs.logits.detach()).max(dim=1)[0].mean()
+            validation_stats["eval_p_var"] += softmax_fn(outputs.logits.detach()).var(dim=1).mean()
+
+        predictions = outputs.logits.argmax(dim=-1) if not is_regression else outputs.logits.squeeze()
+        predictions, references = accelerator.gather((predictions, batch["labels"]))
+        # If we are in a multiprocess environment, the last batch has duplicates
+        if accelerator.num_processes > 1:
+            if step == len(eval_dataloader) - 1:
+                predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]
+                references = references[: len(eval_dataloader.dataset) - samples_seen]
+            else:
+                samples_seen += references.shape[0]
+        metric.add_batch(
+            predictions=predictions,
+            references=references,
+        )
+    
+    # Calculate local metrics
+    validation_stats["eval_p_max"] = validation_stats["eval_p_max"] / len(eval_dataloader)
+    validation_stats["eval_p_var"] = validation_stats["eval_p_var"] / len(eval_dataloader)
+    validation_stats["eval_loss"] = validation_stats["eval_loss"] / len(eval_dataloader)
+    
+    if accelerator.num_processes > 1:
+        # Calculate global metrics
+        validation_stats["eval_p_max"] = accelerator.gather(validation_stats["eval_p_max"]).mean()
+        validation_stats["avg_eval_p_var"] = accelerator.gather(validation_stats["avg_eval_p_var"]).mean()
+        validation_stats["eval_loss"] = accelerator.gather(validation_stats["eval_loss"]).mean()
+
+    validation_stats = validation_stats | metric.compute() # HF evaluate metrics always compute global metric
+
+    return validation_stats
+
+
 def main():
+    cuda_visible_devices = os.getenv('CUDA_VISIBLE_DEVICES', torch.cuda.current_device())
     args = parse_args()
-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The
-    # information sent is the one passed as arguments along with your Python/PyTorch versions.
-    send_example_telemetry("run_glue_no_trainer", args)
+    CHECKPOINT_TEMPLATE =  f'd{cuda_visible_devices}_tmp/'+'checkpoint_{name}'
+
+    if args.dataset_seed is None:
+        args.dataset_seed = args.seed
 
     # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.
     # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers
@@ -239,20 +537,7 @@
 
     # Handle the repository creation
     if accelerator.is_main_process:
-        if args.push_to_hub:
-            if args.hub_model_id is None:
-                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)
-            else:
-                repo_name = args.hub_model_id
-            create_repo(repo_name, exist_ok=True, token=args.hub_token)
-            repo = Repository(args.output_dir, clone_from=repo_name, token=args.hub_token)
-
-            with open(os.path.join(args.output_dir, ".gitignore"), "w+") as gitignore:
-                if "step_*" not in gitignore:
-                    gitignore.write("step_*\n")
-                if "epoch_*" not in gitignore:
-                    gitignore.write("epoch_*\n")
-        elif args.output_dir is not None:
+        if args.output_dir is not None:
             os.makedirs(args.output_dir, exist_ok=True)
     accelerator.wait_for_everyone()
 
@@ -307,14 +592,54 @@
     #
     # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently
     # download model & vocab.
-    config = AutoConfig.from_pretrained(args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name)
-    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)
+    config = AutoConfig.from_pretrained(
+        args.model_name_or_path,
+        num_labels=num_labels,
+        finetuning_task=args.task_name,
+        hidden_dropout_prob = args.insert_dropout,
+        attention_probs_dropout_prob = args.insert_dropout
+        )
+    # config = AutoConfig.from_pretrained("bert-base-uncased", num_labels=num_labels, finetuning_task=args.task_name)
+    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)
     model = AutoModelForSequenceClassification.from_pretrained(
         args.model_name_or_path,
         from_tf=bool(".ckpt" in args.model_name_or_path),
         config=config,
         ignore_mismatched_sizes=args.ignore_mismatched_sizes,
     )
+    softmax = torch.nn.Softmax(dim=1)
+
+    # Get the metric function
+    if args.task_name is not None:
+        metric = evaluate.load("glue", args.task_name)
+    else:
+        metric = evaluate.load("accuracy")
+
+    #++++++++++++++++++ create early stopping callback  ++++++++++++++++++++++++#
+    # create early stopping for minimizing evaluators
+    early_stoppings = [
+        EarlyStoppingCallback(
+            metric_name='eval_loss',
+            direction='min',
+            min_delta=args.early_stopping_min_delta,
+            patience=args.early_stopping_patience
+        )
+    ]
+
+    # create early stopping for maximizing evaluators
+    metric.add_batch(predictions=[1,1,0,0],references=[1,0,1,0])
+    metric_names = list(metric.compute().keys())
+    for metric_name in metric_names:
+        early_stoppings.append(
+            EarlyStoppingCallback(
+                metric_name=metric_name,
+                direction='max',
+                min_delta=args.early_stopping_min_delta,
+                patience=args.early_stopping_patience
+            )
+        )
+    #++++++++++++++++++ \create early stopping callback ++++++++++++++++++++++++#
+
 
     # Preprocessing the datasets
     if args.task_name is not None:
@@ -386,13 +711,57 @@
             remove_columns=raw_datasets["train"].column_names,
             desc="Running tokenizer on dataset",
         )
+    ########### Modify datasets #############
+    combined_dataset = concatenate_datasets([
+        processed_datasets['train'],
+        processed_datasets["validation_matched" if args.task_name == "mnli" else "validation"]
+        ])
+
+    # Generate the training evaluation split
+    EVALUATION_FRAC = 0.2
+    
+    # Split between evaluation and training indices
+    evaluation_indices, train_indices, _, _ = train_test_split(
+            range(len(combined_dataset)), # dummy indices
+            combined_dataset["labels"], #
+            stratify = combined_dataset["labels"],
+            train_size = EVALUATION_FRAC,
+            random_state = 0
+        )
+
+    all_labels = np.array(combined_dataset["labels"])
+    if args.training_size != 1.0: # create subset of complete training set if desired
+        target_train_size = int(args.training_size) if args.training_size > 1 else args.training_size
+        train_indices, _, _, _ = train_test_split(
+                train_indices, # dummy indices
+                all_labels[train_indices],
+                stratify = all_labels[train_indices],
+                train_size = target_train_size,
+                random_state = args.dataset_seed
+            )
 
-    train_dataset = processed_datasets["train"]
-    eval_dataset = processed_datasets["validation_matched" if args.task_name == "mnli" else "validation"]
+    validation_indices, test_indices, _, _ = train_test_split(
+            evaluation_indices, # dummy indices
+            all_labels[evaluation_indices], #
+            stratify = all_labels[evaluation_indices],
+            train_size = 0.5,
+            random_state = 0
+        )
+
+    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes
+    if len(train_indices) >= total_batch_size: 
+        train_dataset = Subset(combined_dataset, train_indices)
+    else: # too few training samples to fill a batch
+        missing_factor = int(total_batch_size/len(train_indices))
+        train_dataset = ConcatDataset([Subset(combined_dataset,train_indices)]*missing_factor)
+    
+    eval_dataset = Subset(combined_dataset,validation_indices)
+    test_dataset = Subset(combined_dataset,test_indices)
+
+    if args.task_name == "mnli":
+        eval_dataset_mismatched = processed_datasets["validation_mismatched"]
+    ########### \Modify datasets #############
 
-    # Log a few random samples from the training set:
-    for index in random.sample(range(len(train_dataset)), 3):
-        logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")
 
     # DataLoaders creation:
     if args.pad_to_max_length:
@@ -406,9 +775,14 @@
         data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))
 
     train_dataloader = DataLoader(
-        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size
+        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size,
+        pin_memory=True
     )
     eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)
+    test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)
+
+    if args.task_name == "mnli":
+        eval_dataloader_mismatched = DataLoader(eval_dataset_mismatched, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)
 
     # Optimizer
     # Split weights in two groups, one with weight decay and the other not.
@@ -423,15 +797,21 @@
             "weight_decay": 0.0,
         },
     ]
-    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)
+    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, betas=(args.beta1,args.beta2))
 
     # Scheduler and math around the number of training steps.
     overrode_max_train_steps = False
-    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader))
     if args.max_train_steps is None:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
         overrode_max_train_steps = True
 
+    if args.warmup_steps_fraction is not None:
+        if not (0 <= args.warmup_steps_fraction <= 1):
+            raise ValueError(f'`warmup_steps_fraction` has to be a float in the interval [0,1]') 
+        args.num_warmup_steps = int(args.warmup_steps_fraction * args.max_train_steps)
+        overrode_num_warmup_steps = True
+
     lr_scheduler = get_scheduler(
         name=args.lr_scheduler_type,
         optimizer=optimizer,
@@ -440,176 +820,282 @@
     )
 
     # Prepare everything with our `accelerator`.
-    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
-        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
-    )
+    if args.task_name == 'mnli':
+        model, optimizer, train_dataloader, eval_dataloader, eval_dataloader_mismatched, test_dataloader, lr_scheduler = accelerator.prepare(
+            model, optimizer, train_dataloader, eval_dataloader, eval_dataloader_mismatched, test_dataloader, lr_scheduler
+        )
+    else:
+        model, optimizer, train_dataloader, eval_dataloader, test_dataloader, lr_scheduler = accelerator.prepare(
+            model, optimizer, train_dataloader, eval_dataloader, test_dataloader, lr_scheduler
+        )
+        
+    # Prepare the fumbrella method
+    use_modded = not (args.catch_dropout is None)
+
+    if use_modded:
+        # layer_to_attach_to = model.dropout
+        layer_to_attach_to = model.classifier
+        fumbrella = Fumbrella(
+            module = layer_to_attach_to,
+            model = model,
+            batch_size = args.per_device_train_batch_size,
+            stage1_dropout = args.catch_dropout,
+            stage2_dropout = args.insert_dropout,
+            position = 'output'        
+        )
 
     # We need to recalculate our total training steps as the size of the training dataloader may have changed
-    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader))
     if overrode_max_train_steps:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
     # Afterwards we recalculate our number of training epochs
     args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
 
-    # Figure out how many steps we should save the Accelerator states
-    checkpointing_steps = args.checkpointing_steps
-    if checkpointing_steps is not None and checkpointing_steps.isdigit():
-        checkpointing_steps = int(checkpointing_steps)
-
     # We need to initialize the trackers we use, and also store our configuration.
     # The trackers initializes automatically on the main process.
+    experiment_config = vars(args)
     if args.with_tracking:
-        experiment_config = vars(args)
         # TensorBoard cannot log Enums, need the raw value
         experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
-        accelerator.init_trackers("glue_no_trainer", experiment_config)
+        if (run_name := os.getenv("WANDB_RUNNAME")) is None:
+            if args.output_dir is not None:
+                run_name = "/".join(args.output_dir.split("/")[-2:])
+            else:
+                run_name = "run_" + datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
+        
+        if (tags:= os.getenv("WANBD_TAGS")) is None:
+            tags = []
+        else:
+            tags = json.loads(tags)
 
-    # Get the metric function
-    if args.task_name is not None:
-        metric = evaluate.load("glue", args.task_name)
-    else:
-        metric = evaluate.load("accuracy")
+        accelerator.init_trackers(project_name="fantastic-umbrella",
+                                  config=experiment_config,
+                                  init_kwargs={"wandb": {"entity": "Ricu",
+                                                         "name": run_name,
+                                                         "tags" : tags
+                                                         },
+                                               })
+        if accelerator.is_main_process:
+            wandb_tracker = accelerator.get_tracker("wandb").tracker
+            wandb_tracker.watch(model,log_freq=50)
 
     # Train!
-    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
+    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes
 
     logger.info("***** Running training *****")
     logger.info(f"  Num examples = {len(train_dataset)}")
     logger.info(f"  Num Epochs = {args.num_train_epochs}")
     logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
-    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
-    logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
+    logger.info(f"  Total train batch size (w. parallel, distributed) = {total_batch_size}")
     logger.info(f"  Total optimization steps = {args.max_train_steps}")
     # Only show the progress bar once on each machine.
     progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
     completed_steps = 0
     starting_epoch = 0
-    # Potentially load in the weights and states from a previous save
-    if args.resume_from_checkpoint:
-        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
-            accelerator.print(f"Resumed from checkpoint: {args.resume_from_checkpoint}")
-            accelerator.load_state(args.resume_from_checkpoint)
-            path = os.path.basename(args.resume_from_checkpoint)
-        else:
-            # Get the most recent checkpoint
-            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
-            dirs.sort(key=os.path.getctime)
-            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
-        # Extract `epoch_{i}` or `step_{i}`
-        training_difference = os.path.splitext(path)[0]
-
-        if "epoch" in training_difference:
-            starting_epoch = int(training_difference.replace("epoch_", "")) + 1
-            resume_step = None
-            completed_steps = starting_epoch * num_update_steps_per_epoch
-        else:
-            # need to multiply `gradient_accumulation_steps` to reflect real steps
-            resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
-            starting_epoch = resume_step // len(train_dataloader)
-            resume_step -= starting_epoch * len(train_dataloader)
-            completed_steps = resume_step // args.gradient_accumulation_step
-
-    # update the progress_bar if load from checkpoint
-    progress_bar.update(completed_steps)
 
+    # set up training stats helper
+    if args.evaluation_steps is None:
+        args.evaluation_steps = len(train_dataloader)
+    train_stats_helper = TrainStatsHelper(
+        evaluation_steps = args.evaluation_steps
+    )
+    # do initial evaluation of model as sanity check
+    validation_stats = evaluate_model(
+        model=model,
+        eval_dataloader=eval_dataloader,
+        accelerator=accelerator,
+        metric=metric,
+        softmax_fn=softmax,
+        is_regression=is_regression,
+    )
+    if args.with_tracking:
+        accelerator.log(
+            {"epoch": -1} | validation_stats,
+            step=completed_steps,
+        )
+    logger.info(f"epoch initial model: {validation_stats}")
+    
     for epoch in range(starting_epoch, args.num_train_epochs):
-        model.train()
-        if args.with_tracking:
-            total_loss = 0
-        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:
-            # We skip the first `n` batches in the dataloader when resuming from a checkpoint
-            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
-        else:
-            active_dataloader = train_dataloader
-        for step, batch in enumerate(active_dataloader):
+        for batch in train_dataloader:
+            train_stats = {}
+            model.train()
+            optimizer.zero_grad()
+
+            # STAGE1
+            if use_modded:
+                fumbrella.set_stage(1)
+                outputs = model(**batch)
+                loss = outputs.loss
+                accelerator.backward(loss)
+                train_stats.update({
+                    "train_st1_loss" : loss.detach().float(),
+                    "train_st1_p_max" : softmax(outputs.logits.detach()).max(dim=1)[0].mean()
+                })
+                fumbrella.set_stage(2)
+            # STAGE 2
             outputs = model(**batch)
             loss = outputs.loss
-            # We keep track of the loss at each epoch
-            if args.with_tracking:
-                total_loss += loss.detach().float()
-            loss = loss / args.gradient_accumulation_steps
+            train_stats.update({
+                "train_st2_loss" : loss.detach().float(),
+                "train_st2_p_max" : softmax(outputs.logits.detach()).max(dim=1)[0].mean(),
+            })
+            train_stats_helper.add_stats(train_stats)
+
             accelerator.backward(loss)
-            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
-                optimizer.step()
-                lr_scheduler.step()
-                optimizer.zero_grad()
-                progress_bar.update(1)
-                completed_steps += 1
-
-            if isinstance(checkpointing_steps, int):
-                if completed_steps % checkpointing_steps == 0:
-                    output_dir = f"step_{completed_steps }"
-                    if args.output_dir is not None:
-                        output_dir = os.path.join(args.output_dir, output_dir)
-                    accelerator.save_state(output_dir)
+            optimizer.step()
+            lr_scheduler.step()
+            optimizer.zero_grad()
+            progress_bar.update(1)
+            completed_steps += 1
+
+            if args.with_tracking:
+                accelerator.log({"learning_rate" : lr_scheduler.get_last_lr()[-1]})
+
+            # Evaluation, tracking and early stopping in given interval
+            if completed_steps % args.evaluation_steps == 0:
+                logger.info(f"epoch {epoch}: evaluating model...")
+                validation_stats = evaluate_model(
+                    model=model,
+                    eval_dataloader=eval_dataloader,
+                    accelerator=accelerator,
+                    metric=metric,
+                    softmax_fn=softmax,
+                    is_regression=is_regression,
+                )
+
+                if args.task_name == 'mnli':
+                    validation_stats_mm = evaluate_model(
+                        model=model,
+                        eval_dataloader=eval_dataloader_mismatched,
+                        accelerator=accelerator,
+                        metric=metric,
+                        softmax_fn=softmax,
+                        is_regression=is_regression,
+                    )
+                    validation_stats_mm = {f'{k}_mm' : v for k,v in validation_stats_mm.items()}
+                    validation_stats.update(validation_stats_mm)
+                    
+                logger.info(f"evaluation complete, results: {validation_stats}")
+                computed_training_stats = train_stats_helper.compute_stats()
+                train_stats_helper.initialize_stats()
+
+                diff_metrics = {}
+                if use_modded:
+                    diff_metrics = fumbrella.compute_diff_metrics(accelerator)
+                    
+                if args.with_tracking:
+                    accelerator.log(
+                        {"epoch" : epoch} | validation_stats | computed_training_stats | diff_metrics,
+                        step=completed_steps,
+                    )
+            
+                metrics_ready_to_stop = [es.check_early_stopping(validation_stats[es.metric_name],epoch,completed_steps) for es in early_stoppings]
+                if all(metrics_ready_to_stop):
+                    accelerator.set_trigger()
+                    logger.info(f"Stopping early after epoch {epoch}.")
+                    for es in early_stoppings:
+                        logger.info(
+                            'Achieved best value ({}) for metric {} at in epoch {}'.format(
+                                es.best_value,
+                                es.metric_name,
+                                es.best_epoch
+                            )
+                        )
+                # update best values
+                for es in early_stoppings:
+                    if accelerator.is_main_process and es.improved_metric:
+                        if  not args.no_checkpointing:
+                            accelerator.save_state(CHECKPOINT_TEMPLATE.format(name=es.metric_name))
+                        if args.with_tracking:
+                                wandb_tracker.summary[f"best_{es.metric_name}"] = es.best_value
+                                wandb_tracker.summary[f"best_{es.metric_name}_step"] = es.best_step
+                                wandb_tracker.summary[f"best_{es.metric_name}_epoch"] = es.best_epoch
+            
+            ### Stop model if neccessary
+            if accelerator.check_trigger():
+                accelerator.set_trigger()
+                break
 
             if completed_steps >= args.max_train_steps:
                 break
 
-        model.eval()
-        samples_seen = 0
-        for step, batch in enumerate(eval_dataloader):
-            with torch.no_grad():
-                outputs = model(**batch)
-            predictions = outputs.logits.argmax(dim=-1) if not is_regression else outputs.logits.squeeze()
-            predictions, references = accelerator.gather((predictions, batch["labels"]))
-            # If we are in a multiprocess environment, the last batch has duplicates
-            if accelerator.num_processes > 1:
-                if step == len(eval_dataloader) - 1:
-                    predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]
-                    references = references[: len(eval_dataloader.dataset) - samples_seen]
-                else:
-                    samples_seen += references.shape[0]
-            metric.add_batch(
-                predictions=predictions,
-                references=references,
+        
+        # Do additional evaluation if final epoch and have some remaining steps
+        if (epoch == args.num_train_epochs-1) and ((remaining_steps := completed_steps%args.evaluation_steps) != 0):
+            # collect validation results
+            validation_stats = evaluate_model(
+                model=model,
+                eval_dataloader=eval_dataloader,
+                accelerator=accelerator,
+                metric=metric,
+                softmax_fn=softmax,
+                is_regression=is_regression,
             )
 
-        eval_metric = metric.compute()
-        logger.info(f"epoch {epoch}: {eval_metric}")
+            if args.task_name == 'mnli':
+                validation_stats_mm = evaluate_model(
+                    model=model,
+                    eval_dataloader=eval_dataloader_mismatched,
+                    accelerator=accelerator,
+                    metric=metric,
+                    softmax_fn=softmax,
+                    is_regression=is_regression,
+                )
+                validation_stats_mm = {f'{k}_mm' : v for k,v in validation_stats_mm.items()}
+                validation_stats.update(validation_stats_mm)
 
-        if args.with_tracking:
-            accelerator.log(
-                {
-                    "accuracy" if args.task_name is not None else "glue": eval_metric,
-                    "train_loss": total_loss.item() / len(train_dataloader),
-                    "epoch": epoch,
-                    "step": completed_steps,
-                },
-                step=completed_steps,
-            )
+            # collect training results
+            computed_training_stats = train_stats_helper.compute_stats(avg_factor=remaining_steps)
 
-        if args.push_to_hub and epoch < args.num_train_epochs - 1:
-            accelerator.wait_for_everyone()
-            unwrapped_model = accelerator.unwrap_model(model)
-            unwrapped_model.save_pretrained(
-                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
-            )
-            if accelerator.is_main_process:
-                tokenizer.save_pretrained(args.output_dir)
-                repo.push_to_hub(
-                    commit_message=f"Training in progress epoch {epoch}", blocking=False, auto_lfs_prune=True
+            diff_metrics = {}
+            if use_modded:
+                diff_metrics = fumbrella.compute_diff_metrics(accelerator)
+        
+            if args.with_tracking: 
+                accelerator.log(
+                    {"epoch" : epoch} | validation_stats | diff_metrics | computed_training_stats,
+                    step=completed_steps,
+                )
+            metrics_ready_to_stop = [es.check_early_stopping(validation_stats[es.metric_name],epoch,completed_steps) for es in early_stoppings]
+            if args.with_tracking:
+                for es in early_stoppings:
+                    if es.improved_metric and accelerator.is_main_process:
+                        wandb_tracker.summary[f"best_{es.metric_name}"] = es.best_value
+                        wandb_tracker.summary[f"best_{es.metric_name}_step"] = es.best_step
+                        wandb_tracker.summary[f"best_{es.metric_name}_epoch"] = es.best_epoch
+
+        ### Stop model if neccessary
+        if accelerator.check_trigger():
+            break
+    if not args.no_checkpointing and accelerator.is_main_process:
+        for es in early_stoppings:    
+            accelerator.load_state(CHECKPOINT_TEMPLATE.format(name=es.metric_name))
+            model.eval()
+            for step, batch in enumerate(test_dataloader):
+                outputs = model(**batch)
+                predictions = outputs.logits.argmax(dim=-1)
+                metric.add_batch(
+                    predictions=accelerator.gather(predictions),
+                    references=accelerator.gather(batch["labels"]),
                 )
 
-        if args.checkpointing_steps == "epoch":
-            output_dir = f"epoch_{epoch}"
-            if args.output_dir is not None:
-                output_dir = os.path.join(args.output_dir, output_dir)
-            accelerator.save_state(output_dir)
+            test_metric = metric.compute()
+            if args.with_tracking and accelerator.is_main_process:
+                for k,v in test_metric.items():
+                    wandb_tracker.summary[f'test_{k}_best_{es.metric_name}'] = v
 
     if args.with_tracking:
         accelerator.end_training()
 
-    if args.output_dir is not None:
-        accelerator.wait_for_everyone()
-        unwrapped_model = accelerator.unwrap_model(model)
-        unwrapped_model.save_pretrained(
-            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
-        )
-        if accelerator.is_main_process:
-            tokenizer.save_pretrained(args.output_dir)
-            if args.push_to_hub:
-                repo.push_to_hub(commit_message="End of training", auto_lfs_prune=True)
+    # if args.output_dir is not None:
+    #     accelerator.wait_for_everyone()
+    #     unwrapped_model = accelerator.unwrap_model(model)
+    #     unwrapped_model.save_pretrained(
+    #         args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
+    #     )
+    #     if accelerator.is_main_process:
+    #         tokenizer.save_pretrained(args.output_dir) 
+
 
     if args.task_name == "mnli":
         # Final evaluation on mismatched validation set
@@ -631,11 +1117,5 @@
         eval_metric = metric.compute()
         logger.info(f"mnli-mm: {eval_metric}")
 
-    if args.output_dir is not None:
-        all_results = {f"eval_{k}": v for k, v in eval_metric.items()}
-        with open(os.path.join(args.output_dir, "all_results.json"), "w") as f:
-            json.dump(all_results, f)
-
-
 if __name__ == "__main__":
     main()

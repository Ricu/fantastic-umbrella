{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import pandas as pd\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "# Extraction function\n",
    "def tflog2pandas(path):\n",
    "    runlog_data = pd.DataFrame({\"metric\": [], \"value\": [], \"step\": []})\n",
    "    try:\n",
    "        event_acc = EventAccumulator(path)\n",
    "        event_acc.Reload()\n",
    "        tags = event_acc.Tags()[\"scalars\"]\n",
    "        for tag in tags:\n",
    "            event_list = event_acc.Scalars(tag)\n",
    "            values = list(map(lambda x: x.value, event_list))\n",
    "            step = list(map(lambda x: x.step, event_list))\n",
    "            r = {\"metric\": [tag] * len(step), \"value\": values, \"step\": step}\n",
    "            r = pd.DataFrame(r)\n",
    "            runlog_data = pd.concat([runlog_data, r])\n",
    "    # Dirty catch of DataLossError\n",
    "    except Exception:\n",
    "        print(\"Event file possibly corrupt: {}\".format(path))\n",
    "        traceback.print_exc()\n",
    "    return runlog_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_df(df):\n",
    "    df_pivot = df.pivot(index = \"step\", columns = \"metric\", values = [\"value\"])\n",
    "    df_pivot = df_pivot.droplevel(level=0,axis=1)\n",
    "    df_pivot.columns.name = None\n",
    "    return df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import json\n",
    "\n",
    "for path,dirs,files in os.walk(\"G:\\\\Meine Ablage\\\\Masterarbeit\\\\fantastic-umbrella\\\\finished_runs\\\\04_mod_runs\"):\n",
    "    for file in fnmatch.filter(files,'run_overview.json'):\n",
    "        df = tflog2pandas(path + \"\\\\glue_no_trainer\")\n",
    "        df = pivot_df(df)\n",
    "        df.index = df.index.astype(\"int\")\n",
    "        df.epoch = df.epoch.astype(\"int\")\n",
    "        df.to_csv(path + \"\\\\tensorboard_data.csv\")\n",
    "\n",
    "        \n",
    "        file_path = os.path.abspath(os.path.join(path,file))\n",
    "        print(f'Found file at: {file_path}')\n",
    "        with open(file_path, 'r') as f:\n",
    "            d = json.load(f)\n",
    "\n",
    "        if \"accuracy\" in df.columns:\n",
    "            d[\"max_acc\"] = df[\"accuracy\"].max()\n",
    "        else:\n",
    "            d[\"max_acc\"] = None\n",
    "\n",
    "        if \"matthews_correlation\" in df.columns:\n",
    "            d[\"max_matthews\"] = df[\"matthews_correlation\"].max()\n",
    "        else:\n",
    "            d[\"max_matthews\"] = None\n",
    "             \n",
    "        \n",
    "        d[\"min_evalLoss\"] = df[\"eval_loss\"].min()\n",
    "        d[\"min_evalLoss_step\"] = int(df[\"eval_loss\"].idxmin())\n",
    "        d[\"min_evalLoss_epoch\"] = int(df.loc[d[\"min_evalLoss_step\"],\"epoch\"])\n",
    "\n",
    "        d[\"min_trainLoss\"] = df[\"train_loss\"].min()\n",
    "        d[\"min_trainLoss_step\"] = int(df[\"train_loss\"].idxmin())\n",
    "        d[\"min_trainLoss_epoch\"] = int(df.loc[d[\"min_trainLoss_step\"],\"epoch\"])\n",
    "        \n",
    "        new_file_path = os.path.abspath(os.path.join(path,'run_overview_extended.json'))\n",
    "        with open(new_file_path, 'w') as f:\n",
    "            json.dump(d,f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
